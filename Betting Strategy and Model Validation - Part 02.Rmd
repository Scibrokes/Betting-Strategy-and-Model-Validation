---
title: "Betting Strategy and Ⓜodel Validation - Part II"
subtitle: "Betting Model Analysis on Sportsbook Consultancy Firm A"
author: "[®γσ, Lian Hu](https://englianhu.github.io/) <img src='figure/ENG.jpg' width='24'> <img src='figure/ShirotoNorimichi2.jpg' width='24'> 白戸則道®"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html:
    toc: yes
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r libs, message = FALSE, warning = FALSE, cache = TRUE, include = FALSE}
## Setup Options, Loading Required Libraries and Preparing Environment
## Loading the packages and setting adjustment
suppressMessages(library('BBmisc'))
suppressAll(library('utils'))
suppressAll(source('./function/libs.R'))
```

# Abstract

  This is an academic research by apply R statistics analysis to an agency A of an existing betting consultancy firm A. According to the *Dixon and Pope (2004)*^[Kindly refer to 24th paper in [Reference for industry knowdelege and academic research portion for the paper.] in **7.4 References**], due to business confidential and privacy I am also using agency A and firm A in this paper. **The purpose of the anaysis is measure the staking model of the firm A**. For more sample which using R for Soccer Betting see <http://rpubs.com/englianhu>. Here is the references of [rmarkdown](http://rmarkdown.rstudio.com/authoring_basics.html) and [An Introduction to R Markdown](http://rpubs.com/mansun_kuo/24330). You are welcome to read the *Tony Hirst (2014)*^[Kindly refer to 1st paper in [Reference for technical research on programming and coding portion for the paper.] in **7.4 References**] if you are getting interest to write a data analysis on Sports-book.

# [1. Introduction to the Betting Stategics](http://rstudio-pubs-static.s3.amazonaws.com/208637_3c45a2408ed94b9e8620b01714a4af41.html#introduction-to-the-betting-stategics)

  - Section [1.1 Introducing Betting Strategies](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#introducing-betting-strategies) - Introduce Betting Strategies
  - Section [1.2 Value Betting](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#value-betting) - Odds Price and Overrounds Changared by Bookmakers
  - Section [1.3 Professional Gambler](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#professional-gambler) - Punters' life and How Hedge Fund Works

# [2. Data](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#data)

  - Section [2.1 Collect and Reprocess the Data](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#collect-and-reprocess-the-data) - Data from Firm A
  - Section [2.2 Overrounds / Vigorish](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#overrounds-vigorish) - Odds Price and Overrounds Changared by Bookmakers

```{r read-data1, message = FALSE, warning = FALSE, echo = FALSE, results = 'asis'}
## Load saved dataset to save the loading time.
## directly load the dataset from running chunk `read-data-summary-table` and also chunk `scrap-data`. 
## The spboData for filtering leagues and matches scores purpose. Kindly refer to file named 
## `compacted data - shinyData.txt` inside folder `data`.

## Run above codes and save.images() and now directly load for shinyApp use.
load('./regressionApps/shinyData.RData', envir = .GlobalEnv)

## -------- chunk `bank-roll` -----------
## Re-categorise the soccer financial settlement date. Due to I have no the history matches dataset from bookmakers. The scrapped spbo time is not stable (always change, moreover there just an information website) where firm A is the firm who placed bets with millions HKD (although the kick-off time might also changed after placed that particular bet), therefore I follow the kick-off time of the firm A.
#'@ dat <- dat[order(dat$DateUK),] %>% mutate(DateUS = as.Date(format(DateUK, tz = 'EST', usetz = TRUE, format = '%Y-%m-%d %H:%M:%S'))) #daily settlement will base on variable `DateUS`.
```

# [3. Summarise the Staking Model](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#summarise-the-staking-model)

  - Section [3.1 Summarise Diversified Periodic Stakes](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#summarise-diversified-periodic-stakes) - Summarise the Stakes and Return
  - Section [3.2 Summarise the Staking Handicap](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#summarise-the-staking-handicap) - Summarise the Staking Handicap Breakdown
  - Section [3.3 Summarise the Staking Prices](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#summarise-the-staking-prices) - Summarise the Staking Price Range Breakdown
  - Section [3.4 Summarise the In-Play Staking Timing](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#summarise-the-in-play-staking-timing) - Summarise the In-Play Staking Breakdown by Time Range
  - Section [3.5 Summarise the In-Play Staking Based on Current Score](http://rstudio-pubs-static.s3.amazonaws.com/208637_8bb1bbc4930a4a22bf579a9b205eaea5.html#summarise-the-in-play-staking-based-on-current-score) - Summarise the In-Play Staking Breakdown by Current Score

# 4. Staking Ⓜodel

  - Section [4.1 Basic Equation] - Analyse the Odds Price and Probabilities
  - Section [4.2 Linear Ⓜodel] - Reversed Engineer to get the EMOdds derived from Stakes
  - Section [4.3 Kelly Ⓜodel] - Test the Kelly Model.
    + Section [4.3.1 Basic Kelly Ⓜodel] - Basic Kelly model.
    + Section [4.3.2 Weight Function] - Fractional Kelly model.
    + Section [4.3.3 weighted Fractional Kelly Ⓜodel] - Weighted Fractional Kelly model.
    + Section [4.3.4 Dynamic Fractional Kelly Ⓜodel] - Dynamic Weighted Fractional Kelly model.
    + Section [4.3.5 Bank Roll] - Bank roll listing across the Kelly models.
  - Section [4.4 Poisson Ⓜodel] - Soccer Scores Odds Modelling.
    + Section [4.4.1 Niko Marttinen (2001)] - Creating a Profitable Sportsbook Betting Modelling.
    + Section [4.4.2 Dixon and Coles (1996)] - Odds Modelling and Betting Stretagy.
    + Section [4.4.3 ®γσ, Eng Lian Hu (2016)] - Odds Modelling and Testing Efficiency in Sportsbook Betting.
    + Section [4.4.4 Combination Handicap] - Descrete result of handicap and P&L.
  - Section [4.5 Staking Ⓜodel and Ⓜoney Ⓜanagement] - Simulate the staking model.
    + Section [4.5.1 Risk Management]
    + Section [4.5.2 Investors' Fund Refill]
  - Section [4.6 Expectation Ⓜaximization and Staking Simulation] - Enhance by weighted function on Staking model and Simulation.
    + Section [4.6.1 Truncated Bivariate Normal Distribution] - Test the best fit random scoring models.
    + Section [4.6.2 Resampling Scores and Stakes] - Application of resampling method on Kelly portfolio to get the optimal return from investment.

## 4.1 Basic Equation

  Before we start modelling, we look at the summary of investment return rates.

```{r data-return-summary-table1, message = FALSE, warning = FALSE, echo = FALSE, results = 'asis'}
## Load package again due to cannot find the function
suppressMessages(library('lubridate'))
suppressMessages(library('tidyverse'))
suppressMessages(library('plyr'))
suppressMessages(library('dplyr'))
suppressMessages(library('magrittr'))
suppressMessages(library('formattable'))
suppressMessages(library('htmltools'))

## Get the investment return rates per annun
## http://www.math.ku.dk/~rolf/teaching/thesis/DixonColes.pdf
## value rRates is based on annual EMProb/netProb ratio, while EMProb get from equation 4.1.2
m <- ddply(dat, .(Sess), summarise, Stakes = sum(Stakes), PL = sum(PL), n = length(Sess), rRates = PL / Stakes) %>% mutate(Stakes = currency(Stakes), PL = currency(PL), n = accounting(n, format = 'd'), rRates = percent(rRates)) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "Profit and Loss of Investment"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Annual Stakes and Profit and Loss of Firm A at Agency A (2011-2015) ($0,000)")), 
  as.htmlwidget(m %>% formattable(list(
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    PL = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'green', 'gray')), x ~ paste0(round(x, 2), ' (rank: ', sprintf('%02d', rank(-x)), ')')), 
    
    n = color_tile('white', '#9B870C'), 
    
    rRates = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'green', 'gray')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))))))
```

*table 4.1.1* : `r paste0(dim(m), collapse = ' x ')` : *Return of annually investment summary table.*^[Kindly refer to the list of colors via [Dark yellow with hexadecimal color code #9B870C](http://encycolorpedia.com/9b870c) for plot the stylist table.]

<br>

$$\Re = \sum_{i=1}^{n}\rho_{i}^{EM}/\sum_{i=1}^{n}\rho_{i}^{BK} \cdots equation\ 4.1.1$$

  $\Re$ is the edge or so call advantage for an investment. The $\rho_i^{EM}$ is the estimated probabilities which is the calculated by firm A from match 1,2... until $n$ matches while $\rho_{i}^{BK}$ is the net/pure probability (real odds) offer by bookmakers after we fit the *equation 4.1.2* into *equation 4.1.1*.

$$\rho_i = P_i^{Lay} / (P_i^{Back} + P_i^{Lay}) \cdots equation\ 4.1.2$$

$P_i^{Back}$ and $P_i^{Lay}$ is the backed and layed fair price offer by bookmakers.

  We can simply apply equation above to get the value $\Re$. From the table above we know that the EMPrice calculated by firm A **invested** at a threshold edge (price greater) `r m$rRates` than the prices offer by bookmakers. There are some description about $\Re$ on *Dixon and Coles (1996)*^[Kindly refer to 25th paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**]. The optimal value of $\rho_{i}$ (`rEMProbB`) will be calculated based on bootstrapping/resampling method in section [4.3 Kelly Ⓜodel].

  Now we look at the result of the soccer matches prior to filter out for further modelling from this section.

```{r data-return-summary-table2, echo = FALSE, results = 'asis'}
## http://www.statmethods.net/stats/frequencies.html
## prop.table, margin.table
## margin.table(table(dat$Result), 1)
##
## library('gmodels')
## CrossTable(dat$HCap, dat$Result)
##
suppressMessages(library('dplyr'))
suppressMessages(library('formattable'))
suppressMessages(library('htmltools'))

m1 <- ddply(dat, .(Result), summarise, Stakes = sum(Stakes), Return = sum(Return), Rates = Return / Stakes, n = length(Sess)) %>% tbl_df %>% mutate(S.prop = round(Stakes / sum(Stakes), 4), R.prop = round(Return / sum(Return), 4), prop = round(n / sum(n), 4))
tv <- c('Total', colSums(m1[-1])); tv[4] <- as.numeric(tv[3]) / as.numeric(tv[2])
m1 <- suppressWarnings(rbind(m1, tv))
m1$Result <- factor(c(as.character(m1$Result)[-length(m1$Result)], 'Total')); rm(tv)
m1 %<>% purrr::map_if(is.character, as.numeric) %>% data.frame %>% tbl_df %>% mutate(Stakes = currency(Stakes), Return = currency(Return), Rates = percent(Rates), n = accounting(n, format = 'd'), S.prop = percent(S.prop), R.prop = percent(R.prop), prop = percent(prop))
#'@ m %>% kable(caption = 'Table 4.1.3 : Summary of Betting Results')

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "Profit and Loss of Investment"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes and Profit and Loss of Firm A at Agency A (2011~2015) ($0,000)")), 
  as.htmlwidget(m1 %>% formattable(list(
    
    #'@ Stakes = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%.2f (rank: %.0f)', x, rank(-x))),
    Stakes = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ paste0(round(x, 2), ' (rank: ', sprintf('%02d', rank(-x)), ')')), 
    
    #'@ Return = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%.2f (rank: %.0f)', x, rank(-x))),
    Return = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ paste0(round(x, 2), ' (rank: ', sprintf('%02d', rank(-x)), ')')), 
    
    Rates = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %.0f)', 100 * x, rank(-x))),
    
    n = color_tile('white', 'green'),
    
    S.prop = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %2d)', 100 * x, rank(-x))),
    
    R.prop = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %.0f)', 100 * x, rank(-x))),
    
    prop = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %.0f)', 100 * x, rank(-x)))
))))
```

*table 4.1.2* : `r paste0(dim(m1), collapse = ' x ')` : *Summary of betting results.*

<br>

  The table above summarize the stakes and return on soccer matches result. <s> Well, below table list the handicaps placed by firm A on agency A. Due to the **Cancelled** result is null observation in descrete data modelling and we cannot be count into our model. Here Ifilter out the particular observation from the data from here and now the total observation of the dataset became `r nrow(dat)`.
</s>

**CORRECTION :** need to keep the cancelled matches as the "push" to count in the probability of the cancellation betslip as well which is occurred in real life.

```{r filtered-data, eval = FALSE, echo = FALSE, results = 'asis'}
## Filtered the cancelled or voided bets to avoid null observation and bias.
## 
## CORRECTION : need to keep the cancelled matches as the "push" to count in the probability of the cancellation betslip as well which is occurred in real life.

if('Cancelled' %in% dat$Result){
  dat %<>% filter(Result != 'Cancelled') %>% mutate(Result = factor(Result))
} else dat %<>% mutate(Result = factor(Result))
```

```{r data-prob-table2, echo = FALSE, results = 'asis'}
## http://www.math.ku.dk/~rolf/teaching/thesis/DixonColes.pdf
## value R.rates is based on annual EMProb/netProb ratio, while EMProb get from equation 4.1.2
## Please refer to function arrfirmDatasets()
#'@ dat %<>% mutate(rEMProbB = round(unlist(lapply(split(dat, dat$Sess), function(x) rep(m$rRates, nrow(x)))) * netProbB, 6), rEMProbL = round(1 - rEMProbB, 6))

# Kelly criterion
# Advantages = (prob of win * decimal odds) + (prob of lose * -1)
# Optimal Kelly wager % = Advantages / decimal odds

#'@ dat$rRates <- rep(m$rRates, count(dat, Sess)$n) #unable found 'Sess' when knit but workable when run chunk-by-chunk.
dat$rRates <- rep(as.numeric(m$rRates + 1), ddply(dat, .(Sess), summarise, n = length(Sess))$n)
dat$rEMProbB <- round(dat$rRates * dat$netProbB, 6)
dat$rEMProbL <- round(1 - dat$rEMProbB, 6)
dat$netEMEdge <- round(dat$rEMProbB / dat$netProbB, 6)

dat %>% select(No.x, EUPrice, HKPrice, fHKPriceL, fMYPriceB, fMYPriceL, netProbB, netProbL, rRates, rEMProbB, rEMProbL, netEMEdge, favNetProb, undNetProb) %>% .[sample(1:nrow(.), 6), ] %>% formattable %>% as.htmlwidget
```

*table 4.1.3* : `r paste0(dim(dat), collapse = ' x ')` : *Odds price and probabilities sample table.*

<br>

  Above table list a part of sample odds prices and probabilities of soccer match $i$ while $n$ indicates the number of soccer matches. We can know the values `rEMProbB`, `netProbB` and so forth.

```{r data-prob-plot2, eval = FALSE, message = FALSE, warning = FALSE, echo = FALSE, results = 'asis'}
## Linear model
## Learn about API authentication here: https://plot.ly/r/getting-started
## Find your api_key here: https://plot.ly/settings/api

#'@ model <- lm(rEMProbB ~ netProbB, data=dat)
#'@ grid <- with(dat, expand.grid(netProbB = seq(min(netProbB), max(netProbB), length = 20),
#'@                                 rEMProbB = seq(min(rEMProbB), max(rEMProbB), length = 20)))
#'@ grid$rEMProb <- stats::predict(model, newdata=grid)
#'@ viz2 <- qplot(netProbB, rEMProbB, data=dat) + geom_point(data=grid)
#'@ out <- ggplotly(viz2)
#'@ plotly_url <- out$response$url

#'@ ggplot(dat, aes(x = netProbB, y = rEMProbB)) + geom_point(aes(y = netProbB, color = 'netProbB')) + geom_point(aes(y = rEMProbB, color = 'rEMProbB')) + xlab('Bookmaker Backed Net Prob') + ylab('Calculated EM Backed Prob') + theme_economist(base_family='Verdana') + scale_colour_economist() + ggtitle('Bookmaker Net Probabilities -vs- Firm A EM Probabilities')

gvis.options <- list(title = "Bookmaker Net Probabilities -vs- Firm A EM Probabilities", series = "[{targetAxisIndex:0},{targetAxisIndex:1}]", hAxis = "{title:'Bookmaker Backed Net Prob'}", vAxis = "{title:'rEMProbB'},{title:'netProbB'}", pointSize = 2, width = 'automatic', height = 'automatic', gvis.editor = 'Edit me!', gvis.plot.tag = 'chart')

#'@ plot4.1.1 <- gvisLineChart(xvar = 'netProbB', yvar = 'rEMProbB', data = dat, options = gvis.options)
plot4.1.1 <- googleVis::gvisScatterChart(dat[c('netProbB', 'rEMProbB')], options = gvis.options)
plot(plot4.1.1)

rm(plot4.1.1)
```

```{r data-prob-plot2b, message = FALSE, warning = FALSE, echo = FALSE, results = 'asis'}
## Due to gvisScatterChart inside `data-prob-plot2` keep plot javascript output, here I use alternate `highcharter` package to plot the graph.

## Load package again due to unable found the function.
suppressMessages(library('highcharter'))

hchart(dat[c('netProbB', 'rEMProbB', 'pMYRange')], 'scatter', x = netProbB, y = rEMProbB, group = pMYRange)
```

*graph 4.1.1* : *A sample graph about the relationship between the investmental probabilities -vs- bookmakers' probabilities.*

<br>

  Graph above shows the probabilities calculated by firm A to back against real probabilities offered by bookmakers over `r nrow(dat)` soccer matches.

  I list the handicap below prior to test the coefficient according to the handicap in next section [4.2 Linear Ⓜodel].

```{r data-preMatch, echo = FALSE, results = 'asis'}
##
## Calculate the proportional model on the results, which are 'Cancelled', 'Half-Loss', 'Half-Win', 'Loss', 'Push', 'Win'.
## Ommited the InPlay matches, filter out the sample data, only Pre-Games matches taken into further calculations.
preData <- filter(dat, InPlay == 'No' & InPlay2 == 'No')
hdp <- sort(unique(as.numeric(as.character(dat$HCap))))

hdp %<>% matrix(., nrow = 8) %>% data.frame %>% tbl_df

hdp %>% formattable(list(
  X1 = formatter('span', style = x ~ style(color = ifelse(x > 0, 'green', 'red')), x ~ icontext(ifelse(x > 0, x, -x), ifelse(x, x, x))), 
  X2 = formatter('span', style = x ~ style(color = ifelse(x > 0, 'green', 'red')), x ~ icontext(ifelse(x > 0, x, -x), ifelse(x, x, x))), 
  X3 = formatter('span', style = x ~ style(color = ifelse(x > 0, 'green', 'red')), x ~ icontext(ifelse(x > 0, x, -x), ifelse(x, x, x))), 
  X4 = formatter('span', style = x ~ style(color = ifelse(x > 0, 'green', 'red')), x ~ icontext(ifelse(x > 0, x, -x), ifelse(x, x, x))), 
  X5 = formatter('span', style = x ~ style(color = ifelse(x > 0, 'green', 'red')), x ~ icontext(ifelse(x > 0, x, -x), ifelse(x, x, x))), 
  X6 = formatter('span', style = x ~ style(color = ifelse(x > 0, 'green', 'red')), x ~ icontext(ifelse(x > 0, x, -x), ifelse(x, x, x))) 
  )
) %>% as.htmlwidget
```

*table 4.1.4 : `r paste0(dim(hdp), collapse = ' x ')` : The handicap in sample data.*

<br>

## 4.2 Linear Ⓜodel

  From our understanding of staking, the covariates we need to consider should be only odds price since the handicap's covariate has settled according to different handicap of EMOdds.

```{r plot4.2.1, echo = FALSE, eval = FALSE, results = 'asis'}
## Due to rCharts unable print the graph on rmarkdown file but works independently, here I ommit the rPlot.

n1 <- rPlot(Return ~ Stakes, data = dat, color = 'Stakes', type = 'point')
n1$addControls('x', value = 'Stakes', values = names(dat))
n1$addControls('y', value = 'Stakes', values = names(dat))
n1$addControls('color', value = 'Stakes', values = names(dat))
#'@ n1$print(include_assets = TRUE)
#'@ 
n1
```

  Again, I don't pretend to know the correct Ⓜodel, here I simply apply linear model to retrieve the value of *EMOdds* derived from stakes. The purpose of measure the edge overcame bookmakers' vigorish is *to know the levarage of the staking activities onto 1 unit edge of odds price by firm A to agency A*. By refer to *figure 4.4.1*, I includes the models which split the pre-match and in-play ito comparison.

```{r linear-models, echo = FALSE, results = 'asis'}
## Choosing the variables of linear models
## The net probabilities might open diversified handicap, therefore the HCap parameter need to be insert as one of parameter since the return of draw-no-bet, win-half, win-full, loss-half, loss-full (example : 0, 0/0.5, 0.5, 0.5/1, 1) affect the return of investment.
## lm0 indicate matches include prematch and inplay. lm0pm is subsetted with only prematch while lmip subsetted which only inplay matches.

lm0 <- lm(Return ~ Stakes, data = dat)
lm0pm <- lm(Return ~ Stakes, data = dat, subset = c(InPlay == 'No' & InPlay2 == 'No'))
lm0ip <- lm(Return ~ Stakes, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm1 <- lm(Return ~ Stakes + HCap, data = dat)
lm1pm <- lm(Return ~ Stakes + HCap, data = dat, subset = c(InPlay == 'No' & InPlay2 == 'No'))
lm1ip <- lm(Return ~ Stakes + HCap, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm2 <- lm(Return ~ Stakes + netProbB, data = dat)
lm2pm <- lm(Return ~ Stakes + netProbB, data = dat, subset = c(InPlay == 'No' & InPlay2 == 'No'))
lm2ip <- lm(Return ~ Stakes + netProbB, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm3 <- lm(Return ~ Stakes + HCap + netProbB, data = dat)
lm3pm <- lm(Return ~ Stakes + HCap + netProbB, data = dat, subset = c(InPlay == 'No' & InPlay2 == 'No'))
lm3ip <- lm(Return ~ Stakes + HCap + netProbB, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm4 <- lm(Return ~ Stakes + ipRange, data = dat)
lm4ip <- lm(Return ~ Stakes + ipRange, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm5 <- lm(Return ~ Stakes + ipHCap, data = dat)
lm5ip <- lm(Return ~ Stakes + ipHCap, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm6 <- lm(Return ~ Stakes + HCap + ipRange, data = dat)
lm6ip <- lm(Return ~ Stakes + HCap + ipRange, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm7 <- lm(Return ~ Stakes + CurScore + ipHCap, data = dat)
lm7ip <- lm(Return ~ Stakes + CurScore + ipHCap, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm8 <- lm(Return ~ Stakes + CurScore + ipRange, data = dat)
lm8ip <- lm(Return ~ Stakes + CurScore + ipRange, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm9 <- lm(Return ~ Stakes + CurScore + ipRange + ipHCap, data = dat)
lm9ip <- lm(Return ~ Stakes + CurScore + ipRange + ipHCap, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

## Linear Interative Effect Models
lm10 <- lm(Return ~ HCap + netProbB + HCap:netProbB, data = dat)
lm10pm <- lm(Return ~ HCap + netProbB + HCap:netProbB, data = dat, subset = c(InPlay == 'No' & InPlay2 == 'No'))
lm10ip <- lm(Return ~ HCap + netProbB + HCap:netProbB, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm11 <- lm(Return ~ Stakes + ipHCap + ipRange + ipHCap:ipRange, data = dat)
lm11ip <- lm(Return ~ Stakes + ipHCap + ipRange + ipHCap:ipRange, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm12 <- lm(Return ~ Stakes + CurScore + ipRange + ipHCap + CurScore:ipHCap:ipRange, data = dat)
lm12ip <- lm(Return ~ Stakes + CurScore + ipRange + ipHCap + CurScore:ipHCap:ipRange, data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

## Linear Mixed Effects Models
## Mixed effect categorised the parameters by group... similar with current score during living betting modelling, the scoring rate (intensity of scores) during 0-0 is different with scoring rates during 1-0 etc.
lm13 <- lm(Return ~ Stakes + (1|HCap), data = dat)
lm13pm <- lm(Return ~ Stakes + (1|HCap), data = dat, subset = c(InPlay == 'No' & InPlay2 == 'No'))
lm13ip <- lm(Return ~ Stakes + (1|HCap), data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lm14 <- lm(Return ~ HCap + (1|Stakes), data = dat) # the stakes amount placed by firm A must be based on the degree of the edges to punter. Here I try to test the effect. (Although firm A might bet via several agents, here I can only took available sample from population to test the efficiency.)
lm14pm <- lm(Return ~ HCap + (1|Stakes), data = dat, subset = c(InPlay == 'No' & InPlay2 == 'No'))
lm14ip <- lm(Return ~ HCap + (1|Stakes), data = dat, subset = c(InPlay != 'No' & InPlay2 != 'No'))

lms <- list(lm0 = lm0, lm0pm = lm0pm, lm0ip = lm0ip, lm1 = lm1, lm1pm = lm1pm, lm1ip = lm1ip, lm2 = lm2, lm2pm = lm2pm, lm2ip = lm2ip, lm3 = lm3, lm3pm = lm3pm, lm3ip = lm3ip, lm4 = lm4, lm4ip = lm4ip, lm5 = lm5, lm5ip = lm5ip, lm6 = lm6, lm6ip = lm6ip, lm7 = lm7, lm7ip = lm7ip, lm8 = lm8, lm8ip = lm8ip, lm9 = lm9, lm9ip = lm9ip, lm10 = lm10, lm10pm = lm10pm, lm10ip = lm10ip, lm11 = lm11, lm11ip = lm11ip, lm12 = lm12, lm12ip = lm12ip, lm13 = lm13, lm13pm = lm13pm, lm13ip = lm13ip, lm14 = lm14, lm14pm = lm14pm, lm14ip = lm14ip)

rm(lm0, lm0pm, lm0ip, lm1, lm1pm, lm1ip, lm2, lm2pm, lm2ip, lm3, lm3pm, lm3ip, lm4, lm4ip, lm5, lm5ip, lm6, lm6ip, lm7, lm7ip, lm8, lm8ip, lm9, lm9ip, lm10, lm10pm, lm10ip, lm11, lm11ip, lm12, lm12ip, lm13, lm13pm, lm13ip, lm14, lm14pm, lm14ip)
```

<s>
```{r lm-summary, echo = FALSE, eval = FALSE, results = 'asis'}
## There has a package htmlTable as well, you can compare among xtable, stargazer, htmlTable, formattable and knit::kable().
## https://cran.r-project.org/web/packages/htmlTable/vignettes/tables.html

## Kindly refer to below *shinyapp 4.2.1* as I wrote as an shiny app.

#'@ anova to compare the models
#'@ anova(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12, lm13, lm14, test = 'F')

## xtable always shows LaTeX output but not table.
#'@ list(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12, lm13, lm14) %>% llply(., function(x) print(xtable(x), floating = TRUE, type = 'html'))

#'@ list(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12, lm13, lm14) %>% llply(., stargazer, type = 'html')
stargazer(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12, lm13, lm14, type = 'html')

#'@ list(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12, lm13, lm14) %>% llply(., texreg)
```

```{r lm-anova, echo = FALSE, eval = FALSE, results = 'asis'}
## Kindly refer to below *shinyapp 4.2.1* as I wrote as an shiny app.

## xtable always shows LaTeX output but not table.
#'@ list(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12, lm13, lm14) %>% llply(., function(x) print(xtable(anova(x)), floating = TRUE, type = 'html'))

#'@ list(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12, lm13, lm14) %>% llply(., function(x) stargazer(anova(x), type = 'html'))

#'@ stargazer(anova(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12 lm13, lm14), type = 'html')

stargazer(anova(lm0, lm1, lm2, lm3, lm4), type = 'html')
#'@ list(lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12) %>% llply(stargazer, anova, type = 'html')
stargazer(anova(lm5), type = 'html')
stargazer(anova(lm6), type = 'html')
stargazer(anova(lm7), type = 'html')
stargazer(anova(lm8), type = 'html')
stargazer(anova(lm9), type = 'html')
stargazer(anova(lm10), type = 'html')
stargazer(anova(lm11), type = 'html')
stargazer(anova(lm12), type = 'html')
stargazer(anova(lm13), type = 'html')
stargazer(anova(lm14), type = 'html')

#'@ list(lm0, lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8, lm9, lm10, lm11, lm12, lm13, lm14) %>% llply(., function(x) texreg(anova(x)))
```
</s>

  When I used to work in 188Bet and Singbet as well as AS3388, we know from the experience which is the odds price of favorite team win will be the standard reference and the draw odds will adjust a little bit while the underdog team will be ignore.

  *Steven Xu (2013)*^[Kindly refer to 16th paper in [Reference for industry knowdelege and academic research portion for the paper.]] has do a case study on the comparison of the efficiency of opening and closing price of NFL and College American Football Leagues and get to know the closing price is more efficient and accurate compare to opening price nowadays compare to years 1980~1990. It might be due to multi-million dollars of stakes from informed traders or smart punters to tune up the closing price to be likelihood.

  In order to test the empirical clichés, I used to conduct a research thoroughly through *®γσ, Eng Lian Hu (2016)*^[Kindly refer to 3rd paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**, I completed the research on year 2010 but write the thesis in year 2016.] and concludes that the opening price of Asian Handicap and also Goal Lines of 29 bookmakers are efficient than mine. However in my later *®γσ, Eng Lian Hu (2014)*^[Kindly refer to 4th paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**] applied Kelly staking model where made a return of more than 30% per sesson. Meanwhile, the *Dixon and Coles (1996)* and *Crowder, Dixon, Ledford and Robinson (2001)*^[Kindly refer to 27th paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**] has built two models which compare the accuracy of home win, draw and away win. From a normal Poison model reported the home win is more accurate and therefore an add-hoc inflated parameter required in order to increase the accuracy of prediction. You are feel free to learn about the *Dixon and Coles (1996)* in section [4.4 Poisson Ⓜodel].

  Based on *table 2.2.1* we know about the net bookies probabilities and EM probabilities, here I simply apply *linear regression model*^[You can learn from [Linear Regression in R (R Tutorial 5.1 to 5.11)](https://www.youtube.com/watch?v=66z_MRwtFJM). You can also refer to [Getting Started with Mixed Effect Models in R](http://www.r-bloggers.com/getting-started-with-mixed-effect-models-in-r/), [A very basic tutorial for performing linear mixed effects analyses](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/A%20Very%20Basic%20Tutorial%20for%20Performing%20Linear%20Mixed%20Effects%20Analyses.pdf) and [Fitting Linear Mixed-Effects Models using lme4](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Fitting%20Linear%20Mixed-Effects%20Models%20Using%20lme4.pdf). Otherwise you can read [Linear Models with R](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Linear%20Models%20with%20R.pdf) and somemore details about regression models via [Extending the Linear Model with R : Generalized Linear, Mixed Effects and Nonparametric Regression Models](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Extending%20the%20Linear%20Model%20with%20R%20-%20Generalized%20Linear%2C%20Mixed%20Effects%20and%20Nonparametric%20Regression%20Models.pdf). Besides, [What statistical analysis should I use?](http://www.ats.ucla.edu/stat/mult_pkg/whatstat/) summarise a table for test analysis and data validation. [Fit models to data](https://www.zoology.ubc.ca/~schluter/R/fit-model/) provides examples for application of linear regression and model selection, the main model-fitting commands covered *lm (linear models for fixed effects)*, *lme (linear models for mixed effects)*, *glm (generalized linear models)*, *nls (nonlinear least squares)*, *gam (generalized additive models)* and also *visreg (to visualize model fits)*. The answer from [How to use R anova() results to select best model?](http://stats.stackexchange.com/questions/172782/how-to-use-r-anova-results-to-select-best-model) eleborates the use of ANOVA and [AIC criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) to choose the best fit model. [How to Choose the Best Regression Model](http://blog.minitab.com/blog/adventures-in-statistics/how-to-choose-the-best-regression-model) describes how to find the best regresion model to fit and applicable to the real world. [ANOVA - Model Selection](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/ANOVA%20-%20Model%20Selection.pdf) summarised a lecture notes in slideshow while [Model Selection in R](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Model%20Selection%20in%20R.pdf) conducts a research on model selection for non-nested linear and polynomial models.] and also anova to compare among the models.

![*shinyapp 4.2.1 : WDW-AH convertion and summary and anova of linear models.* Kindly click on [*regressionApps*](https://beta.rstudioconnect.com/content/1807)^[You might select Y response variable and X explanatory variable(s) to measure your model (Refer to [Shiny height-weight example](runGist("https://gist.github.com/wch/4034323")) for further information about shinyapp for linear models.) or existing models.] to use the ShinyApp.](figure/20160928_021252.gif)

<br>

  Here I simply attached with a Fixed Odds to Asian Handicap's calculator which refer to my *ex-colleague William Chen's*^[My ex-colleague and best friend in sportsbook industry which known since join sportsbook industry year 2005 ------ Telebiz and later Caspo Inc.] spreadsheet version 1.1 in year 2006. You can simply input the home win, draw, away win (in decimal format) as well as the overround to get the conversion result from the simple an basic equation.^[Kindly refer to my previous research to know the vigorish / overround.]

  From the summary of *shinyapp 4.2.1*, we know the comparison among the models to get the best fitted model.

```{r compare, echo = FALSE, results = 'asis'}
suppressMessages(library('stringr'))

compare <- ldply(lms, function(x) {
    y <- summary(x)$fstatistic
    df <- data.frame(AIC = AIC(x), BIC = BIC(x), t(summary(x)$df), 'p.value' = pf(y[1], y[2], y[3], lower.tail = FALSE))
    names(df) <- c('AIC', 'BIC', 'df', 'residuals', 'df', 'p.value'); df}) %>% data.frame(No = seq(nrow(.)), Cat = ifelse(str_detect(.$.id, 'pm'), 'pm', ifelse(str_detect(.$.id, 'ip'), 'ip', 'all')), .) %>% tbl_df

compare %<>% mutate(delta.AIC = AIC - min(AIC), Loglik.AIC = exp(-0.5 * delta.AIC), weight.AIC = Loglik.AIC/sum(Loglik.AIC), delta.BIC = BIC - min(BIC), Loglik.BIC = exp(-0.5 * delta.BIC), weight.BIC = Loglik.BIC/sum(Loglik.BIC))

form <- ldply(lms, function(x) paste0(x$call[-1], collapse = ' , '))# %>% rename(.id = .id, formula = V1)
names(form) <- c('.id', 'formula')

## Skip below combination to avoid hard to read since long formula column will make data.frame looks weird.
#'@ compare %<>% join(form, ., by = '.id') %>% select(No, Cat, .id, formula, AIC, BIC, df, residuals, p.value, delta.AIC, Loglik.AIC, weight.AIC, delta.BIC, Loglik.BIC, weight.BIC)

## Refer to *figure 4.4.1* to compare the models, between "all" and "pm + ip".
bestlm <- compare %>% filter(p.value < 0.05) %>% group_by(Cat) %>% slice(which.min(BIC)) %>% mutate(AIC = accounting(AIC), BIC = accounting(BIC), df = accounting(df, format = 'd'), residuals = accounting(residuals, format = 'd'), p.value = percent(p.value), delta.AIC = accounting(delta.AIC), delta.BIC = accounting(delta.BIC)) #best model for every single category.

compare %<>% mutate(AIC = accounting(AIC), BIC = accounting(BIC), df = accounting(df, format = 'd'), residuals = accounting(residuals, format = 'd'), p.value = percent(p.value), delta.AIC = accounting(delta.AIC), delta.BIC = accounting(delta.BIC)) %>% split(., .$Cat) #split the category and list the ranking.

form %>% formattable %>% as.htmlwidget
```

*table 4.2.1 : Application of linear regression models to test the effects on staking.*

<br>

```{r compare-A, echo = FALSE, results = 'asis'}
## all soccer matches
compare[[1]] %>% formattable(list(
  AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),

  BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  df = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %02d)', x, rank(-x))),
  
  residuals = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %02d)', x, rank(x))),
  
  p.value = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(x))),
  
  delta.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  delta.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x)))

  )) %>% as.htmlwidget
```

*table 4.2.2A : Best model to test the effects of staking on all soccer matches (includes both pre-match and in-play).*

<br>

```{r compare-B, echo = FALSE, results = 'asis'}
## all soccer matches
compare[[3]] %>% formattable(list(
  AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  df = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %02d)', x, rank(-x))),
  
  residuals = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %02d)', x, rank(x))),
  
  p.value = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(x))),
  
  delta.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  delta.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x)))

  )) %>% as.htmlwidget
```

*table 4.2.2B : Best model to test the effects of staking on pre-match soccer matches.*

<br>

```{r compare-C, echo = FALSE, results = 'asis'}
## all soccer matches
compare[[2]] %>% formattable(list(
  AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  df = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %02d)', x, rank(-x))),
  
  residuals = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %02d)', x, rank(x))),
  
  p.value = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(x))),
  
  delta.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  delta.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x)))

  )) %>% as.htmlwidget
```

*table 4.2.2C : Best model to test the effects of staking on in-play soccer matches.*

<br>

```{r compare2, echo = FALSE, results = 'asis'}
## summary of the best model

bestlm %>% formattable(list(
  AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  df = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(-x))),
  
  residuals = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  p.value = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(x))),
  
  delta.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  delta.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x)))

  )) %>% as.htmlwidget
```

*table 4.2.3 : Best model to test the effects of staking soccer matches.*

<br>

  Base on above few tables and also summarised *table 4.2.3*, we can compare both `r dplyr::filter(bestlm, Cat == 'all') %>% .$'.id'` and `r dplyr::filter(bestlm, Cat != 'all') %>% .$'.id' %>% paste0(., collapse = ' + ')` and decide that the model *`r bestm = ifelse(bestlm$BIC[1] > sum(bestlm$BIC[-1]), dplyr::filter(bestlm, Cat != 'all') %>% .$'.id' %>% paste0(., collapse = ' + '), ifelse(bestlm$AIC[1] > sum(bestlm$AIC[-1]), dplyr::filter(bestlm, Cat != 'all') %>% .$'.id' %>% paste0(., collapse = ' + '), dplyr::filter(bestlm, Cat == 'all') %>% .$'.id')); bestm`*^[BIC will be primary reference while AIC is the secondary reference. The smallest value is the best model. `r paste0('all = ', bestlm$BIC[1], ' and mixed = ', sum(bestlm$BIC[-1]))`] is the best fit to determine the factors and effects to place stakes for all matches^[mixed InPlay + Pre-match, all observations are `r nrow(dat)` soccer matches which has placed bets.]. The timing of InPlay and the stakes amount is the major effects to the return of investment.

  *John Fingleton & Patrick Waldron (1999)* apply Shin's model and finally conclude suggests that bookmakers in Ireland are infinitely risk-averse and balance their books. The authors cannot distinguish between inside information and operating costs, merely concluding that combined they account for up to 3.7% of turnover while normally Asian bookmakers made less than 1% and a anonymous company has made around 2%. However the revenue or the stakes are farly more than European bookmakers.^[You can refer to my another project [Analyse the Finance and Stocks Price of Bookmakers](https://github.com/scibrokes/analyse-the-finance-and-stocks-price-of-bookmakers) which analysis the financial report of public listed companies and also profitable products' revenue and profit & loss of anonymous company.]. 
  
  They compare different versions of our model, using data from races in Ireland in 1993. The authors' empirical results can be summarised as follows:
  
  - They reject the hypothesis that bookmakers behave in a risk neutral manner;
  - They cannot reject the hypothesis that they are infinitely riskaverse;
  - They estimate gross margins to be up to 4 per cent of total oncourse turnover; and
  - They estimate that 3.1 to 3.7% (by value) of all bets are placed by punters with inside information.

![*figure 4.2.1 : Chance of Winning.*](figure/chance-of-winning.jpg)

  Due to the Shin model inside the paper research for the sake of bookmakers and this sportsbook consultancy firm is indeed the informed trading (means smart punters or actuarial hedge fund but not ordinary gambler place bets with luck). Here I think of test our previous data in paper *®γσ, Eng Lian Hu (2016)*^[Kindly refer to 3rd paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References** which collect the dataset of opening and also closing odds price of 40 bookmakers and 29 among them with Asian Handicap and Goal Line. Meanwhile, there has another research on smart punters (*Punters Account Review (Agenda).xlsx*) which make million dollars profit from Ladbrokes. You are feel free to browse over the [dataset](https://www.dropbox.com/sh/ifwczokjptt6re0/AADv1VarJoQ6IgIitZBzG5c6a?dl=0) for the paper.] and also the anonymous companies's revenue and P&L to analyse the portion of smart punters among the customers in [Analyse the Finance and Stocks Price of Bookmakers](https://github.com/scibrokes/analyse-the-finance-and-stocks-price-of-bookmakers). However the betslip of every single bet require to analyse it. The sparkR amd RHadoop as well as noSQL require in order to analyse the multiple millions bets. It is interesting to analyse *the threaten of hedge fund*^[Kindly refer to [富传奇色彩的博彩狙击公司EM2](https://englianhu.wordpress.com/sportsbook/%E5%AF%8C%E4%BC%A0%E5%A5%87%E8%89%B2%E5%BD%A9%E7%9A%84%E5%8D%9A%E5%BD%A9%E7%8B%99%E5%87%BB%E5%85%AC%E5%8F%B8em2-expectation-maximization/) to know the history and the threaten of EM2 sportsbook consultancy company to World wide known bankers.] since there has a anonymous brand among the brands under Caspo Inc had closed due to a lot of smart punters' stakes and made loss. Well, here I leave it for future research^[Here I put in [6.2 Future Works].] if the dataset is available.

## 4.3 Kelly Ⓜodel

![*diagram 4.3.0 : The overview of Kelly models.*](figure/Kelly-models-chart.jpg)

```{r Kelly-models highchart, eval = FALSE, echo = FALSE, results = 'asis'}
library(DiagrammeR)
library(magrittr)

graph <-
  create_graph() %>%
  set_graph_name("DAG") %>%
  set_global_graph_attrs("graph", "overlap", "true") %>%
  set_global_graph_attrs("graph", "fixedsize", "true") %>%
  set_global_graph_attrs("node", "color", "blue") %>%
  set_global_graph_attrs("node", "fontname", "Helvetica") %>%
  add_n_nodes(11) %>%
  select_nodes_by_id(c(1:4, 8:11)) %>% 
  set_node_attrs_ws("shape", "box") %>%
  clear_selection %>%
  select_nodes_by_id(5:7) %>% 
  set_node_attrs_ws("shape", "circle") %>%
  clear_selection %>%
  add_edges_w_string(
    "1->5 2->6 3->9 4->7 5->8 5->10 7->11", "green") %>%
  add_edges_w_string(
    "1->8 3->6 3->11 3->7 5->9 6->10", "red") %>%
  select_edges("rel", "green") %>%
  set_edge_attrs_ws("color", "green") %>%
  invert_selection %>%
  set_edge_attrs_ws("color", "red")

render_graph(graph)
```

### 4.3.1 Basic Kelly Ⓜodel

  From the papers *Niko Marttinen (2001)*^[Kindly refer to 1th paper in [Reference for industry knowdelege and academic research portion for the paper.]] and *Jeffrey Alan Logan Snyder (2013)*^[Kindly refer to 2nd paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**] both applying **Full-Kelly**,**Half-Kelly** and also **Quarter-Kelly** models which similar with my previous Kelly-Criterion model *®γσ, Eng Lian Hu 2014*^[Kindly refer to 4th paper in [Reference for industry knowdelege and academic research portion for the paper.] in **7.4 References** which had applied it and generates an impressive return.] but enhanced. *Niko Marttinen (2001)* has concludes that the basic Kelly criterion generates the highest returns in long run compare to fractional Kelly models.

<iframe src="https://raw.githubusercontent.com/scibrokes/betting-strategy-and-model-validation/8adcccbde5140a4321bf064ef2e065551bc195ed/references/Creating%20a%20Profitable%20Betting%20Strategy%20for%20Football%20by%20Using%20Statistical%20Modelling.pdf" style="width:560px; height:500px;" frameborder="0"></iframe>

*paper 4.3.1.1 : Niko Marttinen (2001)*

<br>

  *diagram 4.3.0* get the idea from above paper which is count the odds price offers by bookmakers into calculation. My previous [Odds Modelling and Testing Inefficiency of Sports Bookmakers](https://github.com/scibrokes/odds-modelling-and-testing-inefficiency-of-sports-bookmakers) odds modelling will be conducted further enhanced beyond next few years. In this staking model, I will also use the idea to measure the weakness of bookmakers but also enhanced our staking strategics. Meanwhile, [Application of Kelly Criterion model in Sportsbook Investment](https://github.com/scibrokes/kelly-criterion)^[We can know from Part I where we can easily make profit from bookmakers but the Part II will enhanced to increase the profit and also the money management.] will use a basket of bookmakers' odds price to simulate it.

<br>

<iframe width="560" height="315" src="https://www.youtube.com/embed/kOvJGcxtIp4" frameborder="0" allowfullscreen></iframe>

*video 4.3.1.1 : Using Kelly Criterion for Trade Sizing*

<br>

<iframe width="560" height="315" src="https://www.youtube.com/embed/-ePZ9WuIrVQ" frameborder="0" allowfullscreen></iframe>

*video 4.3.1.2 : Option Trading - The Kelly criterion formula: Mazimize your growth rate & account utility*

<br>

<iframe width="560" height="315" src="https://www.youtube.com/embed/iykpeZtoNIk" frameborder="0" allowfullscreen></iframe>

*video 4.3.1.3 : The Kelly criterion for trading options*

<br>

  To achieve the level of profitable betting, one must develop a correct money management procedure. The aim for a punter is to maximize the winnings and minimize the losses. If the punter is capable of predicting accurate probabilities for each match, the *Edward O. Thorp (2006)*^[Kindly refer to 6th paper in [Reference for industry knowdelege and academic research portion for the paper.] in **7.4 References**] has proven to work effectively in betting. It was named after an American economist *John Kelly (1956)*^[Kindly refer to 26th paper in [Reference for industry knowdelege and academic research portion for the paper.] in **7.4 References**] and originally designed for information transmission. The Kelly criterion is described below:

![*figure 4.3.1.1 : Kelly criterion formula.*](figure/Kelly.png)

$$S = \frac{\rho_{EM} \times BK_{Decimal\ odds} - 1} {BK_{HK\ odds}} \cdots equation\ 4.3.1.1$$

  - Where $S$ is the stake expressed as a fraction of one's total bankroll.
  - $\rho_{EM}$ is probability of an event to take place and 
  - while $BK_{Decimal\ odds}$ is decimal odds (decimal odds the return rates with capital stakes) and $BK_{HK\ Odds}$ (HK odds is the net profit rates without capital stakes) for an event offered by the bookmaker.

  Due to HK odds or decimal odds start from range $(0,\infty]$ and return will be $[0,\infty]$, therefore logarithmic function required. For Malay odds $[-1,1]$ no need logarithm. Here I switch from *equation 4.3.1.1* to *equation 4.3.1.2* as below.

$$log(S) = log(\rho_{EM}) + log(BK_{Decimal\ odds} - 1) - log(BK_{HK\ odds}) \cdots equation\ 4.3.1.2$$

  Three important properties, mentioned by *Hausch and Ziemba (1994)*^[You can refer to [Efficiency of Racetrack Betting Markets (2008 Preface Edition)](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Efficiency%20of%20the%20Racetrack%20Betting%20Market%20(2008%20Edition).pdf) which is 29th paper in [Reference for industry knowdelege and academic research portion for the paper.] or [*Chapter 18 Efficiency of Sports and Lottery Betting Markets* in **FINANCE**](https://books.google.com.my/books?id=XC4fZPbT1SQC&printsec=frontcover&dq=Handbooks+in+Operations+Research+and+Management+Science+Finance&hl=en&sa=X&ved=0ahUKEwj7lIj5pLLPAhXGVZQKHf_LC-4Q6AEIPjAE#v=onepage&q&f=false) for further study about Hausch and Ziemba's researchs.] arise when using this criterion to determine a proper stake for each bet:

  - It maximizes the asymptotic growth rate of capital
  - Asymptotically, it minimizes the expected time to reach a specified goal
  - It outperforms in the long run any other essentially different strategy almost surely

![*figure 4.3.1.2 : Example of application Kelly criterion.*](figure/Kelly.jpg)

  The criterion is known to economists and financial theorists by names such as the geometric mean maximizing portfolio strategy, the growth-optimal strategy, the capital growth criterion, etc. We will now show that Kelly betting will maximize the expected log utility for sports-book betting.

<s>
```{r data-return-summary-table3, eval = FALSE, echo = FALSE, results = 'asis'}
## Load package again due to cannot find the function
suppressMessages(library('formattable'))

## Get the investment return rates per annun
## http://www.math.ku.dk/~rolf/teaching/thesis/DixonColes.pdf
## value rRates is based on annual EMProb/netProb ratio, while EMProb get from equation 4.1.2
## due to subset doesn't work, here I add the subset function again.
m <- ddply(dat, .(Sess), summarise, Stakes = sum(Stakes), PL = sum(PL), n = length(Sess), rRates = PL / Stakes) %>% mutate(Stakes = currency(Stakes), PL = currency(PL), rRates = percent(rRates)) %>% tbl_df

m %>% formattable(list(

  Stakes = color_tile('white', 'darkgoldenrod'),
  Return = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'green', 'gray')), x ~ sprintf('%.2f (rank: %02d)', x, rank(-x))),
  n = color_tile('white', '#9B870C'),
  rRates = formatter('span', style = x ~ style(color = ifelse(rank(-x) <= 3, 'green', 'gray')), x ~ sprintf('%.4f (rank: %02d)', x, rank(-x)))
)) %>% as.htmlwidget
```

*table 4.3.1.1* : `r paste0(dim(m), collapse = ' x ')` : *Return of annually investment summary table without cancelled bets.*^[the `rRates` is the mean value of annual return rates which is the return divides by stakes but ommit the cancelled/voided bets to avoind the bias.]

  The `rRates` value from table above excludes the `Cancelled` bets. By refer to *equation 4.3.1.2*, now we fit the adge value from *equation 4.1.1* into it to get the `rEMProbB2` and `rEMProbL2` with known staked value $S$^[Although the result will not be accurate due to the we mention at first, the firm A will not only place bets via only agent A. Let say edge of 0.10 and 0.20 also placed maximum bet HKD40000 but the firm A might placed different amount through other agency based on different edge. However based on the stakes we can reverse the optimal EM Odds.] to replace the existing EM value.
</s>^[Initially think of linear modelling and get the mean value, the positive standard deviation value will be counted as edge range and the residuals value will be the different within the stakes across the leagues. It will similar with proportional staking model as states in paper **Good and bad properties of the Kelly criterion** *by MacLean, Thorp and Ziemba (2010)* and concludes that the Full-Kelly model is the best model for long run, you can refer to the reference in [Kelly Criterion - Part II](https://github.com/scibrokes/kelly-criterion) for further understanding.]

$$log(\rho_{EM}) = log(S) + log(BK_{HK\ odds} + 1) - log(BK_{Decimal\ odds}) \cdots equation\ 4.3.1.3$$

  Although the Kelly model is very simple, but we need to seperates the staking based on different leagues or time range to make it applicable to real world. I don't pretend to know the correct model again but guess the applicable model by testing few models and choose the best among them.
  
  We try to apply the *equation 4.3.1.3* to get the Kelly stakes for every single soccer match.

  Due to there have few reference papers conducting few staking strategics and concludes full Kelly model is the best fit and most profitable along the long term investment, here I try to simuulate the half-Kelly and also quadruple-Kelly, as well as double-Kelly staking model etc and get the optimal weighted control parameter.^[There has a reference paper in section 2 of [Application of Kelly Criterion model in Sportsbook Investment](https://github.com/scibrokes/kelly-criterion) has compare few models as well and also provides the pro-and-con of Kelly model in investment. However, Kelly model will be the best across the long term investment.] Besides, there have few papers doing research and also critic on the Kelly model in investment in financial market and also betting market (includes the rebates of the credit market as well), PIMCO's fund manager **Bill Gross** who manage more than one trillion USD funds applied Kelly model for portfolio, **George Soros** and **Warren Buffet** also applied similar theoty or method with Kelly although there has no evidence to proof it. You are feel free to know in later section [4.5 Staking Ⓜodel and Ⓜoney Ⓜanagement]. For further details kindly refer to [Application of Kelly Criterion model in Sportsbook Investment](https://github.com/scibrokes/kelly-criterion).

  Fractional Kelly models are the weight function for Kelly criterion When we talk about weight function in Kelly model. [A Response to Professor Paul A Samuelson's Objections to Kelly Capital Growth Investing](https://github.com/scibrokes/kelly-criterion/blob/master/references/A%20Response%20to%20Professor%20Paul%20A%20Samuelson's%20Objections%20to%20Kelly%20Capital%20Growth%20Investing.pdf) has talk about the investment portfolio and compare the double-Kelly, full-Kelly, half-Kelly, quadruple-Kelly and also proportional betting across different stages of iterations and concludes that the full-Kelly will be the best fit and growth beyond the ages. Well, fractional-Kelly (means double-Kelly, half-Kelly and quadruple-Kelly but not full-Kelly model) models will be elastics and lesser will be more conservative and double-Kelly will be very risky and eventually going to bankcrupt due to the staking leverages ratio is twice of full-Kelly and over the sustainability of capital. and For further details kindly refer to [Application of Kelly Criterion model in Sportsbook Investment](https://github.com/scibrokes/kelly-criterion). Therefore in last basic Kelly we use the full-Kelly within same leagues but due to there has different levels of risk setting across different soccer leagues. Therefore a weight function needed to make the staking strategy flexible, and it is term as Kelly portfolio to diversified the investment.

### 4.3.2 Fractional Kelly Ⓜodel

  Now we try to fit a weight function into basic Kelly model to be fractional Kelly model. I try to use log to test the maximum value of weight parameter. You can just simply use $w = \frac{1}{4}$ or $log(w) = \frac{1}{2}$ while $w$ is a vector. Please be mind that the value greater than 1 will be risky since involve leverage and lesser will be more conservative.

$$log(w_{i}) + log(\rho_{i}) \cdots equation\ 4.3.2.1$$

  From *Niko Marttinen (2001)*, we can know the full-Kelly generates couple times profit compare to fractional Kelly-models. However there has two points need to be enhanced.
  
  - The high risk at the beginning period of investment.
  - Test on different level of edge and concludes that the 145% generates the highest return.

  Below *Fabián Enrique Moya (2012)* also test the fractional Kelly models with diversify money management methods.

<iframe src="https://raw.githubusercontent.com/scibrokes/betting-strategy-and-model-validation/c2da2e5ca09aaf218616045031c9ee4ce3537b18/references/Statistical%20Methodology%20for%20Profitable%20Sports%20Gambling.pdf" style="width:560px; height:500px;" frameborder="0"></iframe>

*paper 4.3.2.1 : Fabián Enrique Moya (2012)*

<br>

```{r lprofile, echo = FALSE, results = 'asis'}

#'@ lRiskProf <- dat[c('League', 'Stakes', 'HCap', 'Result', 'PL')] %>% tbl_df %>% .[sort(.$League), ] %>% unique %>% na.omit %>% mutate(Stakes = currency(Stakes), PL = currency(PL))
lRiskProf <- ddply(dat[c('League', 'Stakes')], .(League), summarise, min = currency(min(Stakes)), mean = currency(mean(Stakes)), median = currency(median(Stakes)), sd = currency(sd(Stakes)), max = currency(max(Stakes))) %>% tbl_df %>% mutate(League = factor(League))

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "League Stakes Profiling"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2011~2015) ($0,000)")), 
  as.htmlwidget(lRiskProf %>% .[sample(1:nrow(.), 6), ] %>% formattable(list(
    mean = color_tile('white', 'darkgoldenrod'), 
    median = color_tile('white', 'darkgoldenrod')
    ))))
```

*table 4.3.2.1* : `r paste0(dim(lRiskProf), collapse = ' x ')` : *League stakes profiling of firm A year 2011~2015.*

<br>

  Above league risk profile suppose to stores the maximum bet for every single league but I only randomly select 6 leagues as sample. However due to I've not yet write a function for real time API^[There are a lot of real time XML odds price and staking softwares similar with **4lowin2** which was states at the begining section in Part I] with operators and test the maximum stakes per bet therefore here I reverse the mean value as the baseline stakes for every single league with a certain range of standard deviation for resampling simulation in later section.

#### Stakes based reversed Kelly models

**Basic Fractional Models**

  Stakes based reversed Kelly models are the application of the parameter from reversion of the stakes where add-on some modified version Kelly models. I tried to adjust the stakes to get the outcome of PL result.
  
```{r K1, echo = FALSE, results = 'asis'}
suppressMessages(library('knitr'))
suppressMessages(source('./function/vKelly.R'))

## Kelly Criterion model
##
## Advantages = (prob of win * decimal odds) + (prob of lose * -1)
## Optimal Kelly wager % = Advantages / decimal odds
## From below data we can know the difference between the Kelly reversed rEMProbB

## Applied various fractional Kelly models version I.
##   stakes based reverser models.
if(file.exists('./data/K1.rds')) {
  K1 <- read_rds(path = './data/K1.rds')
} else {
  K1 <- vKelly(dat)
}

kable(summary(K1$Kelly1$data), caption = 'Table 4.3.2.2A : Summary Table of Various Kelly Models (Stakes reversed based models)')
```

*table 4.3.2.2A* : `r paste0(dim(K1$Kelly1$data), collapse = ' x ')` : *Summary of Stakes reversed Kelly models year 2011~2015.*

<br>

  From above table summary, we can know the range of risk management applicable to various adjusted Kelly models. Now we try to compare the Profit & Loss from below table.
  
```{r K1B, echo = FALSE, results = 'asis'}
#'@ suppressMessages(library('formattable'))

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Stakes Reversed based Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2011~2015) ($0,000)")), 
  as.htmlwidget(K1$Kelly1$summary %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.2.2B* : `r paste0(dim(K1$Kelly1$summary), collapse = ' x ')` : *PL of Stakes based reversed Kelly models year 2011~2015.*

<br>

**Mean with Min-Max Range Fractional Models**

  Due to there has no league risk management profile, here I try to use the mean value of stakes on every single league as the baseline and set the min and max value to simulate 100 times.

```{r K1mean1, echo = FALSE, results = 'asis'}
## Applied various fractional Kelly models version II.
##   rEMProb based model without adjust the stakes.
#'@ K1 <- vKelly(mbase = suppressMessages(join(dat, lRiskProf)) %>% mutate(Stakes = mean))

## setup parallel computing
#'@ suppressMessages(library(doParallel))
#'@ registerDoParallel(cores = detectCores())

## simulate 100 times to get the mean value.
#'@ K1mean1 <- lapply(seq(100), function(i) {
#'@   x <- NULL
#'@   y <- mutate(lRiskProf, mean = runif(mean, min, max)) #runif()
#'@   z <- suppressMessages(join(dat, y))
#'@   x[[i]] <- mutate(z, Stakes = mean)
#'@   vKelly(x[[i]])$Kelly1$summary
#'@   }) %>% bind_rows %>% mutate(sm = seq(nrow(.))) %>% ddply(.(Category), summarise, Stakes = mean(Stakes), Return = mean(Return), PL = mean(PL), PL.R = PL / Stakes) %>% mutate(Stakes = currency(Stakes), Return = currency(Return), PL = currency(Return), PL.R = percent(PL.R))

## save above data to easier for loading.
#'@ save(K1mean1, file = './KellyApps/data/K1mean1.rda')
load('./KellyApps/data/K1mean1.rda')

#'@ kable(summary(K1mean1), caption = 'Table 4.3.2.2C : Summary Table of Various Kelly Models (Stakes Reversed based with Mean Stakes Adjusted Models)')
```
<s>
*table 4.3.2.2C* : `r paste0(dim(K1mean1), collapse = ' x ')` : *Summary of Stakes reversed Kelly models (mean value of stakes with min-max range as staking adjuster) year 2011~2015.*

<br>

  From above table summary, we can know the range of risk management applicable to various adjusted Kelly models. </s> Now we try to compare the Profit & Loss from below table.
  
```{r K1mean1B, echo = FALSE, results = 'asis'}
#'@ suppressMessages(library('formattable'))

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed rEMProbB Kelly Models (Mean with min-max Adjusted Stakes)"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2011~2015) ($0,000)")), 
  as.htmlwidget(K1mean1 %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.2.2D* : `r paste0(dim(K1mean1), collapse = ' x ')` : *PL of Stakes reversed Kelly models (mean value of stakes with min-max range as staking adjuster) year 2011~2015.*

<br>

**Mean with sd Range Fractional Models**

  Due to there has no league risk management profile, here I try to use the mean value of stakes on every single league as the baseline.

```{r K1mean2, echo = FALSE, results = 'asis'}
## Applied various fractional Kelly models version II.
##   rEMProb based model without adjust the stakes.
#'@ K1 <- vKelly(mbase = suppressMessages(join(dat, lRiskProf)) %>% mutate(Stakes = median))

## setup parallel computing
#'@ suppressMessages(library(doParallel))
#'@ registerDoParallel(cores = detectCores())

## simulate 100 times to get the mean value.
#'@ K1mean2 <- lapply(seq(100), function(i) {
#'@     x <- NULL
#'@     y <- mutate(lRiskProf, mean = suppressAll(rnorm(mean, sd))) #rnorm()
#'@     z <- suppressMessages(join(dat, y))
#'@     x[[i]] <- mutate(z, Stakes = mean)
#'@     vKelly(x[[i]])$Kelly1$summary
#'@ }) %>% bind_rows %>% mutate(sm = seq(nrow(.))) %>% ddply(.(Category), summarise, Stakes = mean(Stakes), Return = mean(Return), PL = mean(PL), PL.R = PL / Stakes) %>% mutate(Stakes = currency(Stakes), Return = currency(Return), PL = currency(Return), PL.R = percent(PL.R))

## save above data to easier for loading.
#'@ save(K1mean2, file = './KellyApps/data/K1mean2.rda')
load('./KellyApps/data/K1mean2.rda')

#'@ kable(summary(K1mean2), caption = 'Table 4.3.2.2E : Summary Table of Various Kelly Models  (Stakes Reversed based with Mean Stakes Adjusted Models)')
```

*table 4.3.2.2E* : `r paste0(dim(K1mean2), collapse = ' x ')` : *Summary of Stakes reversed Kelly models (mean value of stakes with sd range as staking adjuster) year 2011~2015.*

<br>

  From above table summary, we can know the range of risk management applicable to various adjusted Kelly models.</s> Now we try to compare the Profit & Loss from below table.
  
```{r K1mean2B, echo = FALSE, results = 'asis'}
#'@ suppressMessages(library('formattable'))

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed Stakes based Kelly Models (Mean with sd Adjusted Stakes)"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2011~2015) ($0,000)")), 
  as.htmlwidget(K1mean2 %>% tbl_df %>% formattable(list(
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod')#, 
    
    #PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.2.2F* : `r paste0(dim(K1mean2), collapse = ' x ')` : *PL of Stakes reversed Kelly models year (mean value of stakes with sd range as staking adjuster) 2011~2015.*

<br>

#### Reversed rEMProbB based Kelly models

**Basic Fractional Models**

  `rEMProbB` (real EM Probabilities Back) are the application of the parameter from reversion of the stakes where add-on some modified version Kelly models. For the EM probabilities based models, I had just simply adjusted for staking and get the different outcome of Profit & Loss.

```{r K2, echo = FALSE, results = 'asis'}
## reload the function due to unable found.
suppressMessages(source('./function/vKelly2.R'))

## Applied various fractional Kelly models version II.
##   rEMProb based model without adjust the stakes.
if(file.exists('./data/K2.rds')) {
  K2 <- read_rds(path = './data/K2.rds')
} else {
  K2 <- vKelly2(dat)
}

kable(summary(K2$Kelly1$data), caption = 'Table 4.3.2.3 : Summary Table of Various Kelly Models (reversed rEMProbB based models)')
```

*table 4.3.2.3A* : `r paste0(dim(K2$Kelly1$data), collapse = ' x ')` : *Summary of Reversed rEMProbB Kelly models year 2011~2015.*

<br>

  From above table summary, we can know the range of risk management applicable to various adjusted Kelly models. Now we try to compare the Profit & Loss from below table.

```{r K2B, echo = FALSE, results = 'asis'}
#'@ suppressMessages(library('formattable'))

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed rEMProbB Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2011~2015) ($0,000)")), 
  as.htmlwidget(K2$Kelly1$summary %>% formattable(list(

    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod')#, 
    
    #PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.2.3B* : `r paste0(dim(K2$Kelly1$summary), collapse = ' x ')` : *PL of Reversed rEMProbB Kelly models year 2011~2015.*

<br>

**Mean with Min-Max Range Fractional Models**

  Due to there has no league risk management profile, here I try to use the mean value of stakes on every single league as the baseline and set the min and max value to simulate 100 times.

```{r K2mean1, echo = FALSE, results = 'asis'}
## reload the function due to unable found.
suppressMessages(source('./function/vKelly2.R'))

## Applied various fractional Kelly models version II.
##   rEMProb based model without adjust the stakes.
#'@ K2 <- vKelly2(mbase = suppressMessages(join(dat, lRiskProf)) %>% mutate(Stakes = mean))

## setup parallel computing
#'@ suppressMessages(library(doParallel))
#'@ registerDoParallel(cores = detectCores())

## simulate 100 times to get the mean value.
#'@ K2mean1 <- lapply(seq(100), function(i) {
#'@     x <- NULL
#'@     y <- mutate(lRiskProf, mean = runif(mean, min, max)) #runif()
#'@     z <- suppressMessages(join(dat, y))
#'@     x[[i]] <- mutate(z, Stakes = mean)
#'@     vKelly2(x[[i]])$Kelly1$summary
#'@ }) %>% bind_rows %>% mutate(sm = seq(nrow(.))) %>% mutate(sm = seq(nrow(.))) %>% ddply(.(Category), summarise, Stakes = mean(Stakes), Return = mean(Return), PL = mean(PL), PL.R = PL / Stakes) %>% mutate(Stakes = currency(Stakes), Return = currency(Return), PL = currency(Return), PL.R = percent(PL.R))

## save above data to easier for loading.
#'@ save(K2mean1, file = './KellyApps/data/K2mean1.rda')
load('./KellyApps/data/K2mean1.rda')

#'@ kable(summary(K2), caption = 'Table 4.3.2.3C : Summary Table of Various Kelly Models (Reversed rEMProbB based with Mean Stakes Adjusted models)')
```
<s>
*table 4.3.2.3C* : `r paste0(dim(K2mean1), collapse = ' x ')` : *Summary of Reversed rEMProbB Kelly models year (mean value of stakes with min-max range as staking adjuster) 2011~2015.*

<br>

  From above table summary, we can know the range of risk management applicable to various adjusted Kelly models.</s> Now we try to compare the Profit & Loss from below table.
  
```{r K2mean1B, echo = FALSE, results = 'asis'}
#'@ suppressMessages(library('formattable'))

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed rEMProbB Kelly Models (Mean with Min-Max Adjusted Stakes)"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2011~2015) ($0,000)")), 
  as.htmlwidget(K2mean1 %>% formattable(list(

    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.2.3D* : `r paste0(dim(K2mean1), collapse = ' x ')` : *PL of Reversed rEMProbB Kelly models (mean value of stakes with min-max range as staking adjuster) year 2011~2015.*

<br>

**Mean with sd Range Fractional Models**

  Due to there has no league risk management profile, here I try to use the median value of stakes on every single league as the baseline.

```{r K2mean2, echo = FALSE, results = 'asis'}
## reload the function due to unable found.
suppressMessages(source('./function/vKelly2.R'))

## Applied various fractional Kelly models version II.
##   rEMProb based model without adjust the stakes.
#'@ K2 <- vKelly2(mbase = suppressMessages(join(dat, lRiskProf)) %>% mutate(Stakes = median))

## setup parallel computing
#'@ suppressMessages(library(doParallel))
#'@ registerDoParallel(cores = detectCores())

## simulate 100 times to get the mean value.
#'@ K2mean2 <- lapply(seq(100), function(i) {
#'@     x <- NULL
#'@     y <- mutate(lRiskProf, mean = suppressAll(rnorm(mean, sd))) #rnorm()
#'@     z <- suppressMessages(join(dat, y))
#'@     x[[i]] <- mutate(z, Stakes = mean)
#'@     vKelly2(x[[i]])$Kelly1$summary
#'@ }) %>% bind_rows %>% mutate(sm = seq(nrow(.))) %>% mutate(sm = seq(nrow(.))) %>% ddply(.(Category), summarise, Stakes = mean(Stakes), Return = mean(Return), PL = mean(PL), PL.R = PL / Stakes) %>% mutate(Stakes = currency(Stakes), Return = currency(Return), PL = currency(Return), PL.R = percent(PL.R))

## save above data to easier for loading.
#'@ save(K2mean2, file = './KellyApps/data/K2mean2.rda')
load('./KellyApps/data/K2mean2.rda')

#'@ kable(summary(K2mean2), caption = 'Table 4.3.2.3E : Summary Table of Various Kelly Models (Reversed rEMProbB based with Mean Stakes Adjusted models)')
```
<s>
*table 4.3.2.3E* : `r paste0(dim(K2mean2), collapse = ' x ')` : *Summary of Reversed rEMProbB Kelly models year (mean value of stakes with sd range as staking adjuster) year 2011~2015.*

<br>

  From above table summary, we can know the range of risk management applicable to various adjusted Kelly models.</s> Now we try to compare the Profit & Loss from below table.
  
```{r K2mean2B, echo = FALSE, results = 'asis'}
#'@ suppressMessages(library('formattable'))

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed rEMProbB Kelly Models (Mean with sd Adjusted Stakes)"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2011~2015) ($0,000)")), 
  as.htmlwidget(K2mean2 %>% formattable(list(

    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.2.3F* : `r paste0(dim(K2mean2), collapse = ' x ')` : *PL of Reversed rEMProbB Kelly models (mean value of stakes with sd range as staking adjuster) year 2011~2015.*

<br>

### 4.3.3 Weighted Fractional Kelly Ⓜodels

  In previous section I  measure the data from 2011~2015 as static analysis. Well, now I try to seperates as annum data base and get the optimal weight value for next year use. Due to I dont know if the weight function is needed for staking models since sports consultancy firm had applied Poison models weith weight function. As we know a dice will have $\frac{1}{6}$ chance to open one of the outcome, however theoretical probabilities doesn't not correct since there have papers which applied bernoulli distribution to test the outcome with a certain iteration, the outcome might be 0.499 and 0.501 for over under but not 0.5 for each, that is due to the some effect like the balance of the dice, the flat level of the table, the wind, momentum and etc. I don't pretand to know and only simulate it by obseravation to guess the optimal value.

  Due to fractional Kelly model is independent models (for example : half-Kelly will be half-Kelly staking model, and full-Kelly will be only full-Kelly model across the years as we made comparison in section [4.3.2 Fractional Kelly odel].), now we need to make it weighted fractional model. Similar with my prevous Rmodel which applied on Poisson model. Due to the calculation of the settlement and result on the win and loss of Asian Handicap is different with Fixed odds, the probabilities of the outcome will be descrete and the measurement of the `likelihood` result required in order to maximize the profit. here we need to add an additional parameter controller to adjust the staking amount on every single match.

  Now I try to will simulate an enhanced Kelly model on staking which take the effect of the outcome of the result into calculation below controller parameter $\phi(r)$ fit into $equation\ 4.3.3.1$ to control the leverage ratio.^[similar theory apply on investment portfolio while it might turn to be nested controller parameters across different soccer leagues.]

$$\phi(r) = exp(w_{i}\rho_{i}) \cdots equation\ 4.3.3.1$$
 Where $X = x_{i,2,3...n}$ is the original staking amount by Kelly model. Meanwhile, the $r$ value is the optimal parameter controller for staking.

$$r\begin{Bmatrix}
=& Win\\
=& Half\ Win\\
=& Push\\
=& Half\ Loss\\
=& Loss\end{Bmatrix} \cdots equation\ 4.3.3.2$$

  Here I try to diversified the weight parameters on *equation 4.3.3.2*. The first year data will be the baseline for further years analysis. The $\phi(r)$ function is a contant variable which using previous years's data to apply on current year staking model. You can also use other method to find yours.
  
  Due to we unable foreseen the result before a soccer match started, here I tried to categorise from -1, 0.75, -0.5... 1 as a set of handicap.

<s>
```{r weight-para, echo = FALSE, results = 'asis'}
## ======================================================================
## Added below weight function into the vKelly() and vKelly2()
## ======================================================================
suppressPackageStartupMessages(library('BBmisc'))

## weighted parameter estimation
dat2 <- dat %>% mutate(
  theta = suppressAll(
    ifelse(Result == 'Win', 1, 
    ifelse(Result == 'Half Win', 0.5, 
    ifelse(Result == 'Push'|Result == 'Cancelled', 0, 
    ifelse(Result == 'Half Loss', -0.5, 
    ifelse(Result == 'Loss', -1, NA)))))), 
                  dWin = ifelse(Result == 'Win', 1, 0), 
                  dwhf = ifelse(Result == 'Half Win', 1, 0), 
                  dpus = ifelse(Result == 'Push'|Result == 'Cancelled', 1, 0), 
                  dlhf = ifelse(Result == 'Half Loss', 1, 0), 
                  dlos = ifelse(Result == 'Loss', 1, 0))

#############################################
## Above chunk `weight-para` and below chunk `weight-para-draft` weight parameters are wrong due to we cannot foreseen the result, therefore I rewrote in function leagueRiskProf() to differentiate the weight value for different handicaps.
#############################################
```

*table 4.3.3.1 : sample data of weighted handicap*

<br>

```{r weight-para-draft, eval = FALSE, echo = FALSE, results = 'asis'}
## ======================================================================
## Added below weight function into the vKelly() and vKelly2()
## ======================================================================

## I dont pretand to know the true weight function, here I try both methods bewlo to do the comparison.
## option 1
- $\theta$ = 1    if Win
           = 0.5  if Win-Half
           = 0    if push
           = -0.5 if Loss-Half
           = -1    if Loss

## but above weighted parameter has been applied from the result and PL. The variable `rEMProbB` is a reversed from the weighted parameter which includes the Win, Win-Half, Push, Loss-Half and Loss result. The `rEMPobB` and `rEMPobL` both are reversed by the result and only differentiate the difference of strength (AH) but not the lambda values of both teams. Only can applied AH = Favorite - Underdog, and OU = Home + Away.

> bvp(1.676241, 1.658034, )
[1] 0.08305386
> 1.676241 / 1.658034
[1] 1.010981
> bvp(1.676241, 1.658034, 1.010981)
[1] NA

> mbase %>% mutate(theta = ifelse(Result == 'Win', 1, 
+                             ifelse(Result == 'Half Win', 0.5, 
+                             ifelse(Result == 'Push'|Result == 'Cancelled', 0, 
+                             ifelse(Result == 'Half Loss', -0.5, 
+                             ifelse(Result == 'Loss', -1, NA)))))) %>% select(rEMProbB, netProbB, theta) %>% colMeans
  rEMProbB   netProbB      theta
0.51655378 0.50563275 0.06766533
> 0.51655378 / 0.50563275
[1] 1.021599


## option 2
- 5 dummy variables which are :
           win = ifelse(1, 0)
           win-half = ifelse(1, 0)
           push = ifelse(1, 0)
           loss-half = ifelse(1, 0)
           loss = ifelse(1, 0)

## 
> mbase %>% mutate(theta = ifelse(Result == 'Win', 1, 
+                          ifelse(Result == 'Half Win', 0.5, 
+                          ifelse(Result == 'Push'|Result == 'Cancelled', 0, 
+                          ifelse(Result == 'Half Loss', -0.5, 
+                          ifelse(Result == 'Loss', -1, NA))))), 
+                  dWin = ifelse(Result == 'Win', 1, 0), 
+                  dwhf = ifelse(Result == 'Half Win', 1, 0), 
+                  dpus = ifelse(Result == 'Push'|Result == 'Cancelled', 1, 0), 
+                  dlhf = ifelse(Result == 'Half Loss', 1, 0), 
+                  dlos = ifelse(Result == 'Loss', 1, 0)) %>% select(rEMProbB, netProbB, theta, dWin, dwhf, dpus, dlhf, dlos) %>% colMeans
  rEMProbB   netProbB      theta       dWin       dwhf       dpus       dlhf       dlos 
0.51655378 0.50563275 0.06766533 0.41468761 0.07433930 0.09270491 0.06815248 0.35011570

## 
> mbase %>% mutate(theta = ifelse(Result == 'Win', 1, 
+                          ifelse(Result == 'Half Win', 0.5, 
+                          ifelse(Result == 'Push'|Result == 'Cancelled', 0, 
+                          ifelse(Result == 'Half Loss', -0.5, 
+                          ifelse(Result == 'Loss', -1, NA))))), 
+                  dWin = ifelse(Result == 'Win', 1, 0), 
+                  dwhf = ifelse(Result == 'Half Win', 1, 0), 
+                  dpus = ifelse(Result == 'Push'|Result == 'Cancelled', 1, 0), 
+                  dlhf = ifelse(Result == 'Half Loss', 1, 0), 
+                  dlos = ifelse(Result == 'Loss', 1, 0)) %>% select(rEMProbB, netProbB, theta, dWin, dwhf, dpus, dlhf, dlos) %>% colMeans %>% exp
rEMProbB netProbB    theta     dWin     dwhf     dpus     dlhf     dlos 
1.676241 1.658034 1.070007 1.513898 1.077172 1.097138 1.070529 1.419232

## above parameters just a static weight parameter or constant across a year, you can simulate to get a dynamic vector of weight parameters.
- in order to save the executive time, here I load the saved RData file which has simulate to get the optimal weight value. Now I try to fit the weight parameter into the Kelly model to get the result and doing comparison as below.
```

```{r weight-para2, echo = FALSE, results = 'asis'}
## static weighted parameter estimation
theta <- ddply(dat2, .(Sess), summarise, rRates = percent(mean(rRates)), 
             theta = mean(theta), dWin = mean(dWin), dwhf = mean(dwhf), 
             dpus = mean(dpus), dlhf = mean(dlhf), dlos = mean(dlos)) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "Weighted Value Estimation for Weighted Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Weighted Table (2011~2015)")), 
  as.htmlwidget(theta %>% formattable(list(

    rRates = color_tile('white', 'darkgoldenrod'), 
    theta = color_tile('white', 'darkgoldenrod'), 
    dWin = color_tile('white', 'darkgoldenrod'), 
    dwhf = color_tile('white', 'darkgoldenrod'), 
    dpus = color_tile('white', 'darkgoldenrod'), 
    dlhf = color_tile('white', 'darkgoldenrod'), 
    dlos = color_tile('white', 'darkgoldenrod')
    ))))
```

*table 4.3.3.1* : `r paste0(dim(theta), collapse = ' x ')` : *The static weight parameter from year 2011~2015.*
</s>

<br>

  Above *table 4.3.3.1*^[*table 4.3.3.1* is wrong due to we cannot foreseen the result of a soccer match before kick-off, here I rewrote two kind of weight functions for handicap] parameters just a static weighted parameter or constant across a year, you can simulate by apply the Epectation Maximization to get a dynamic vector of weight parameters across the soccer matches. The later section will conduct a monte carlo simulation from 2011 until 2015 to get the best fit outcome.

#### Stakes based reversed Kelly models

**Weighted Fractional Models**

  Stakes based reversed Kelly models are the application of the parameter from reversion of the stakes where add-on some modified version Kelly models. I tried to adjust by add a constant weight value `theta` to get the outcome of PL result.
  
```{r K1W1, echo = FALSE, results = 'asis'}
suppressMessages(library('knitr'))
suppressMessages(source('./function/vKelly.R'))

## Kelly Criterion model
##
## Advantages = (prob of win * decimal odds) + (prob of lose * -1)
## Optimal Kelly wager % = Advantages / decimal odds
## From below data we can know the difference between the Kelly reversed rEMProbB

## Applied various fractional Kelly models version I.
##   stakes based reverser models.
if(file.exists('./data/K1W1.rds')) {
  K1W1 <- read_rds(path = './data/K1W1.rds')
} else {
  K1W1 <- vKelly(dat, type = 'W1')
}

K1W1sum <- data.frame(K1W1.1 = K1W1$Kelly1$summary, A = '', K1W1.2 = K1W1$Kelly2$summary, B = '', K1W1.3 = K1W1$Kelly3$summary, C = '', K1W1.4 = K1W1$Kelly4$summary) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Stakes Reversed based Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2012~2015) ($0,000)")), 
  as.htmlwidget(K1W1sum %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.3.2A* : `r paste0(dim(K1W1sum), collapse = ' x ')` : *Summary of Stakes reversed weighted 1 Kelly models 1 year 2012~2015.*

<br>

  Now we try to look at a vector `dres` weighted values.
  
```{r K1W2, echo = FALSE, results = 'asis'}
#'@ suppressMessages(library('formattable'))
## Applied various fractional Kelly models version I.
##   stakes based reverser models.
if(file.exists('./data/K1W2.rds')) {
  K1W2 <- read_rds(path = './data/K1W2.rds')
} else {
  K1W2 <- vKelly(dat, type = 'W2')
}

K1W2sum <- data.frame(K1W2.1 = K1W2$Kelly1$summary, A = '', K1W2.2 = K1W2$Kelly2$summary, B = '', K1W2.3 = K1W2$Kelly3$summary, C = '', K1W2.4 = K1W2$Kelly4$summary) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Stakes Reversed based Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2012~2015) ($0,000)")), 
  as.htmlwidget(K1W2sum %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.3.2B* : `r paste0(dim(K1W2sum), collapse = ' x ')` : *Summary of Stakes reversed weighted 2 Kelly models 2 year 2012~2015.*

<br>

#### Reversed rEMProbB based Kelly models

**Weighted Fractional Models**

  `rEMProbB` (real EM Probabilities Back) are the application of the parameter from reversion of the stakes where add-on some modified version Kelly models. For the EM probabilities based models, I had just simply adjusted by added a constant `theta` to get the different outcome of Profit & Loss.

```{r K2W1, echo = FALSE, results = 'asis'}
## reload the function due to unable found.
suppressMessages(source('./function/vKelly2.R'))

## Applied various fractional Kelly models version II.
##   rEMProb based model without adjust the stakes.
if(file.exists('./data/K2W1.rds')) {
  K2W1 <- read_rds(path = './data/K2W1.rds')
} else {
  K2W1 <- vKelly2(dat, type = 'W1')
}

K2W1sum <- data.frame(K2W1.1 = K2W1$Kelly1$summary, A = '', K2W1.2 = K2W1$Kelly2$summary) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed Prob Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2012~2015) ($0,000)")), 
  as.htmlwidget(K2W1sum %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.3.3A* : `r paste0(dim(K2W1sum), collapse = ' x ')` : *Summary of Reversed rEMProbB weighted 1 Kelly models 2 year 2012~2015.*

<br>

  Now we try to look at a vector `dres` weighted values.

```{r K2W2, echo = FALSE, results = 'asis'}
#'@ suppressMessages(library('formattable'))
if(file.exists('./data/K2W2.rds')) {
  K2W2 <- read_rds(path = './data/K2W2.rds')
} else {
  K2W2 <- vKelly2(dat, type = 'W2')
}

K2W2sum <- data.frame(K2W2.1 = K2W2$Kelly1$summary, A = '', K2W2.2 = K2W2$Kelly2$summary) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed Prob Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2012~2015) ($0,000)")), 
  as.htmlwidget(K2W2sum %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.3.3B* : `r paste0(dim(K2W2sum), collapse = ' x ')` : *PL of Reversed rEMProbB weighted 2 Kelly models 2 year 2012~2015.*

<br>

### 4.3.4 Dynamic Fractional Kelly Ⓜodel

#### Comparison

  Due to the weighted models only analyse on year 2012~2015, here I need to summarise the static and weigthed data to do comparison from the profit and loss prior to further section.

  From the tables in section [4.3.2 Fractional Kelly Ⓜodel] and [4.3.3 Weighted Fractional Kelly Ⓜodels] we tried to weighted the Kelly models to increase the profit. Now we try to do a dynamic weighted parameters. For the staking and money management which is portfolio management, I leave it to [4.5 Staking Ⓜodel and Ⓜoney Ⓜanagement].

#### Stakes based reversed Kelly models

```{r K1D1, echo = FALSE, results = 'asis'}
## Applied various fractional Kelly models version I.
##   stakes based reverser models.
if(file.exists('./data/K1D1.rds')) {
  K1D1 <- read_rds(path = './data/K1D1.rds')
} else {
  K1D1 <- vKelly(dat, type = 'D1')
}

K1D1sum <- data.frame(K1D1.1 = K1D1$Kelly1$summary, A = '', K1D1.2 = K1D1$Kelly2$summary, B = '', K1D1.3 = K1D1$Kelly3$summary, C = '', K1D1.4 = K1D1$Kelly4$summary) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Stakes Reversed based Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2012~2015) ($0,000)")), 
  as.htmlwidget(K1D1sum %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.4.1A* : `r paste0(dim(K1D1sum), collapse = ' x ')` : *PL of Reversed stakes dynamic 1 Kelly models 1 year 2012~2015.*

<br>

```{r K1D2, echo = FALSE, results = 'asis'}
if(file.exists('./data/K1D2.rds')) {
  K1D2 <- read_rds(path = './data/K1D2.rds')
} else {
  K1D2 <- vKelly(dat, type = 'D2')
}

K1D2sum <- data.frame(K1D2.1 = K1D2$Kelly1$summary, A = '', K1D2.2 = K1D2$Kelly2$summary, B = '', K1D2.3 = K1D2$Kelly3$summary, C = '', K1D2.4 = K1D2$Kelly4$summary) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Stakes Reversed based Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2012~2015) ($0,000)")), 
  as.htmlwidget(K1D2sum %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.4.1B* : `r paste0(dim(K1D2sum), collapse = ' x ')` : *PL of Reversed stakes dynamic 2 Kelly models 1 year 2012~2015.*

<br>

#### Reversed rEMProbB based Kelly models

```{r K2D1, echo = FALSE, results = 'asis'}
if(file.exists('./data/K2D1.rds')) {
  K2D1 <- read_rds(path = './data/K2D1.rds')
} else {
  K2D1 <- vKelly2(dat, type = 'D1')
}

K2D1sum <- data.frame(K2D1.1 = K2D1$Kelly1$summary, A = '', K2D1.2 = K2D1$Kelly2$summary) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed Prob Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2012~2015) ($0,000)")), 
  as.htmlwidget(K2D1sum %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.4.2A* : `r paste0(dim(K2D1sum), collapse = ' x ')` : *PL of Reversed rEMProbB dynamic 1 Kelly models 2 year 2012~2015.*

<br>

```{r K2D2, echo = FALSE, results = 'asis'}
if(file.exists('./data/K2D2.rds')) {
  K2D2 <- read_rds(path = './data/K2D2.rds')
} else {
  K2D2 <- vKelly2(dat, type = 'D2')
}

K2D2sum <- data.frame(K2D2.1 = K2D2$Kelly1$summary, A = '', K2D2.2 = K2D2$Kelly2$summary) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "PL of Reversed Prob Kelly Models"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Stakes of Firm A at Agency A (2012~2015) ($0,000)")), 
  as.htmlwidget(K2D2sum %>% formattable(list(
    Models = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.6f (rank: %.0f)', x, rank(x))),
    
    Stakes = color_tile('white', 'darkgoldenrod'), 
    
    Return = color_tile('white', 'darkgoldenrod'), 
    
    PL = color_tile('white', 'darkgoldenrod'), 
    
    PL.R = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(-x)))
    ))))
```

*table 4.3.4.2B* : `r paste0(dim(K2D2sum), collapse = ' x ')` : *PL of Reversed rEMProbB dynamic 2 Kelly models 2 year 2012~2015.*

<br>

### 4.3.5 Bank Roll

  There has few points we need to consider, there the we need to retrieve the initial investment capital $BR$:
  
  - risk averse from ruin^[The daily lost cannot over the investment fund, otherwise will be bankrupt before growth, Kelly model is sort of risky at the beginning period but bacame stable as time goes by.]
  - Initial invested capital
  - The differences of time zone (British based sports consultancy firm A)
  - The financial settlement time of Asian operators (daily financial settlement time 12:00PM Hong Kongnese GMT+8 Credit market with rebates)^[Here I follow the kick-off time from GMT+8 1200 until next morning 1159 (or the American Timezone which is GMT - 4) considered as a soccer betting date. Re-categorise the soccer financial settlement date. Due to I have no the history matches dataset from bookmakers. The scrapped spbo time is not stable (always change, moreover there just an information website) where firm A is the firm who placed bets with millions HKD (although the kick-off time might also changed after placed that particular bet), therefore I follow the kick-off time of the firm A.]

```{r bank-roll, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}
suppressAll(source('./function/compareKelly.R'))

plotFund <- compareKelly(K1, chart = TRUE, type = 'multiple')
plotFund
```

*graph 4.3.5.1 : Sample data of bank roll and fund growth for basic Kelly model. (\$1 = \$10,000)*

<br>

  Above Graph is a basic Kelly model, we can know the initial fund size is not united.

  Due to our bank roll cannot be less than 0, otherwie will be ruined. Therefore I added the initial balance of the account from the min value of variable `SPL` which is the balance before place bets must be more than 0. Otherwise unable to place bets. [4.5.1 Risk Management] will united the initial fund size and Kelly portion across the league profiles.
  
  The file [BankRoll.csv](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/data/BankRoll.csv) states the profit and loss of the staking. You can see the 
  
  As I mentioned at the begining of the research paper, the stakes only reflects the profit and loss of agency A but not firm A. Firm A might have deal with 10~50 or even more agencies and the data from year 2011 is not the initial investment year. You are feel free to download the file. We will discuss the inventory management to reduce the risk.
  
  *graph 4.3.5.1* has **Event** label to marking a specific event on a specifi date or time while I just leave it and only mark on high volatily event dates. From [BankRoll.csv](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/data/BankRoll.csv) we observe the end of soccer sesson in May 2011 `dat %>% filter(DateUS >= '2011-05-14' & DateUS <= '2011-05-21')` has a seriourly crash. We can investigate more details about the loss matches from the data (or filter the range of the bets in the data table inside [Part I](http://rpubs.com/englianhu/208637)).

**Comparison of Summarized Kelly Investment Funds**

```{r bank-roll-portfolio, echo = FALSE, message = FALSE, results = 'asis'}
## simulate Kelly 
## - united initial fund size
## - portion arrangement as baseline stakes for league profile
library('DT')
source('./function/readKelly.R', local = TRUE)

if(file.exists('./data/initial.rds')) {
  initial <- read_rds(path = './data/initial.rds')
} else {
  initial <- readKelly(details = 'initial-fund-size')
  saveRDS(initial, file = './data/initial.rds')
}

iniVal <- initial$KM[6] %>% as.character %>% as.numeric

#'@ if(file.exists('./data/BR.rds')) {
#'@   BR <- read_rds(path = './data/BR.rds')
#'@ } else {
#'@   BR <- readKelly(.summary = FALSE)
#'@   saveRDS(BR, file = './data/BR.rds')
#'@ }

## read the summary of the data for comparison.
if(file.exists('./data/BRSummary.rds')) {
  BRSummary <- read_rds(path = './data/BRSummary.rds')
} else {
  BRSummary <- readKelly()
  saveRDS(BRSummary, file = './data/BRSummary.rds')
}

ldply(BRSummary$KM, ldply, as.matrix) %>% datatable(
  caption = "Table 4.3.5.2 : Summary of Kelly Main Funds ('0,000)", 
  escape = FALSE, filter = 'top', rownames = FALSE, 
  extensions = list('ColReorder' = NULL, 'RowReorder' = NULL, 
                    'Buttons' = NULL, 'Responsive' = NULL), 
  options = list(dom = 'BRrltpi', autoWidth = TRUE,  scrollX = TRUE, 
                 lengthMenu = list(c(10, 50, 100, -1), c('10', '50', '100', 'All')), 
                 ColReorder = TRUE, rowReorder = TRUE, 
                 buttons = list('copy', 'print', 
                                list(extend = 'collection', 
                                     buttons = c('csv', 'excel', 'pdf'), 
                                     text = 'Download'), I('colvis'))))
```

*table 4.3.5.2 : Summary of `r length(BRSummary$KMnames)` Kelly main funds.*

  From *table 4.5.1.1* we can know the risk of the all investmental funds. You are feel free to browse over [KellyApps](https://beta.rstudioconnect.com/content/2311/) for more details. You can also refer to [Faster Way of Calculating Rolling Realized Volatility in R](http://stackoverflow.com/questions/12823445/faster-way-of-calculating-rolling-realized-volatility-in-r) to measure the volatility of the fund as here I omit it at this stage.

![*shinyapp 4.3.5.1 : Kelly sportsbook investment fund.* Kindly click on [*KellyApps*](https://beta.rstudioconnect.com/content/2311/)^[The shinyApp contain both basic fund management and also portfolio management which is in later section [4.5.1 Risk Management].] to use the ShinyApp.](figure/KellyApps.jpg)

## 4.4 Poisson Ⓜodel

### 4.4.1 Niko Marttinen (2001)

> Data has been collected over the last four seasons in the English Premier League.
These include 1997-1998, 1998-1999, 1999-2000 and 2000-2001 seasons. We
have also collected the season 2000-2001 data from the main European football
betting leagues, such as English Division 1, Division 2 Division 3, Italian Serie A,
German Bundesliga and Spanish Primera Liga...

*quote 4.4.1.1 : the dataset for the studies (source : Niko Marttinen (2001)).*

  *Niko Marttinen (2001)*^[Kindly refer to 1th paper in [Reference for industry knowdelege and academic research portion for the paper.]] has enhanced the *Dixon and Coles (1996)* which are :
  
  - Basic Poisson model : Independence Poisson model for both home and way teams with a constant home advantage parameter.
  - Independent home advantages model : Seperate the home advantage parameter depends on the teams accordingly.
  - Split season model : Split a soccer league season to be 1st half and 2nd half season.
  - (E) Scores plus Poisson model.

  From above models, the author has compare the efficiency and the best fit model for scores prediction as below.

![*figure 4.4.1.1 : Comparison of various Poison models (source : Niko Marttinen (2001)).*](figure/NikoMarttinen2001-01.jpg)

  From *figure 4.4.1.1* above, the author compare the deviance of the models^[Kindly refer to [Generalized Linear Models in R, Part 2: Understanding Model Fit in Logistic Regression Output](http://www.theanalysisfactor.com/r-glm-model-fit/), [devianceTest](http://www.mathworks.com/help/stats/generalizedlinearmodel.deviancetest.html?s_tid=gn_loc_drop) and [Use of Deviance Statistics for Comparing Models](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Use%20of%20Deviance%20Statistics%20for%20Comparing%20Models.pdf) to learn about the method of comparison.]

![*figure 4.4.1.2 : Comparison of various mixed Poison models II (source : Niko Marttinen (2001)).*](figure/NikoMarttinen2001-02.jpg)

![*figure 4.4.1.3 : Comparison of various mixed Poison models III (source : Niko Marttinen (2001)).*](figure/NikoMarttinen2001-03.jpg)

  From above models, the author list the models and states that even though pick the worst model among the models still more accurate than bookmaker while *E(Score)&Dep&Weighted* is the best.

![*figure 4.4.1.4 : Comparison of various odds modelling models (source : Niko Marttinen (2001)).*](figure/NikoMarttinen2001-04.jpg)

  Besides, *Niko Marttinen (2001)* not only choose Poison model throughly as the odds modelling model but also compare to below models :-
  
  - ELO ratings.
  - multinomial ordered probit model.
  
  He concludes that the multinomial ordered probit model is the best fit model but the software for fitting is not generally available. Meanwhile, the Poisson model is more versatile than probit logit model based on the dataset accross the European soccer leagues.^[There has a lot of papers with regard to application of logit probit models on soccer betting, might read through and made comparison with my **®Model** *®γσ, Eng Lian Hu (2016)*. I used to read though the logit probit and there has a complicated parameters setting for various effects like : wheather, players' condition, couch, pitch condition and even though the distance travel and the players' stamina modelling.]
  
  You can read for more details from *paper 4.3.1.1 : Niko Marttinen (2001)*.

### 4.4.2 Dixon and Coles (1996)

  Here we introduce the *Dixon and Coles (1996)* model and its codes. You are freely learning from below links if interest.

  - [Dixon and Cole's Poisson regression R Packages](http://lastplanetranking.blogspot.com/2013/11/code.html)
  - [Dixon and Coles Poisson model](http://opisthokonta.net/?p=890)
  - [Dixon Coles model - Python](http://www.sportshacker.net/posts/simple_dixon_coles.html)
  - [Predicting Football Using R](http://pena.lt/y/2014/11/02/predicting-football-using-r/)

  Due to the soccer matches randomly getting from different leagues, and also not Bernoulli win-lose result but half win-lose etc as we see from above. Besides, there were mixed Pre-Games and also In-Play soccer matches and I try to filter-up the sample data to be only English soccer leagues as shinyApps. I don't pretend to know the correct answer or the model from firm A. However I take a sample presentation *Robert Johnson (2011)*^[Kindly refer to 23th paper in [7.4 References]] from one of consultancy firm which is Dixon-Coles model and omitted the scoring process section.

### 4.4.3 ®γσ, Eng Lian Hu (2016)

  Below is my previous research paper which was more sophiscated than Dixon-Coles model. You can refer it and I will just omit the section as mentioned at the beginning section of this staking validation research paper.
  
<iframe src="https://raw.githubusercontent.com/scibrokes/odds-modelling-and-testing-inefficiency-of-sports-bookmakers/master/Odds%20Modelling%20and%20Testing%20Inefficiency%20of%20Sports-Bookmakers/Odds_Modelling_and_Testing_Inefficiency_of_Sports-Bookmakers.pdf" style="width:560px; height:500px;" frameborder="0"></iframe>

*paper 4.4.3.1 : ®γσ, Eng Lian Hu (2016)*

<br>

  By refer to [4.4.1 Niko Marttinen (2001)], I've add the enhancement on  [Odds Modelling and Testing Inefficiency of Sports Bookmakers](https://github.com/scibrokes/odds-modelling-and-testing-inefficiency-of-sports-bookmakers) as future improvement while the weighted function might be enhanced soonly.

  Here I cannot reverse computing from barely $\rho_i^{EM}$ without know the $\lambda_{ij}$ and $\gamma$ values. Meanwhile, the staked matches is a descrete random soccer teams accross all leagues and tournaments. Therefore I just simply use reverse EM probabilities by mean value of edge in previous section Kelly.

$$X_{ij} = pois(\gamma \alpha_{ij} \beta_{ij} ); Y_{ij} = pois(\alpha_{ij} \beta_{ij}) \cdots equation\ 4.4.1$$

### 4.4.4 Combination Handicap

  In order to minimzie the risk, I tried to validate the odds price range invested by firm A.^[As I used to work in **AS3388** which always take bets from **Starlizard** where they only placed bets within the odds price range from 0.70 ~ -0.70. They are not placed bets on all odds price in same edge]. The sportbook consulatancy firms might probably not place same amount of stakes on same edge, lets take example as below :-
  
  - $Odds_{em}$ = 0.40 while $Odds_{BK}$ = 0.50, The edge to firm will be 0.5 ÷ 0.4 = `r 0.5/0.4`
  - $Odds_{em}$ = 0.64 while $Odds_{BK}$ = 0.80, The edge to firm will be 0.8 ÷ 0.64 = `r 0.8/0.64`
  
  We know above edge is same but due to the probability of occurance an event/goal at 0.4 is smaller than 0.64. In [4.3.3 Weighted Fractional Kelly Ⓜodels] I tried to use a weight function to measure the effect of Win-All, Win-Half, Push, Loss-Half, Loss and also Cancelled and there has generated a higher profit.
  
  Again, I don't pretend to know the correct models but here I try to simulate the occurance of the combination and independent handicaps by testing below distribution.
  
  - categorise handicap set (from start of previous session until latest soccer matches as Weight function which has talked in [4.3.3 Weighted Fractional Kelly Ⓜodels])
  - normal distribution
  - binomial distribution
  - Poison distribution
  
  Due to there have 110 different weighted Kelly models I test, here I put the application of distribution in specific English soccer league which will compare with my *Rmodel* in [Application of Kelly Criterion model in Sportsbook Investment - Part II](https://github.com/scibrokes/kelly-criterion).

**Comparison Chart**

![*figure 4.4.4.1 : Binomial vs. Poisson*](figure/bnom-vs-pois01.jpg)

![*figure 4.4.4.2 : Difference Between Binomial and Poisson Distribution*](figure/bnom-vs-pois02.jpg)

  You can know about the use of the distribution through below articles.

 - *figure 4.4.4.1* : [Binomial vs. Poisson](http://www2.cedarcrest.edu/academic/bio/hale/biostat/session12links/BvsP.html)
 - *figure 4.4.4.2* : [Difference Between Binomial and Poisson Distribution](http://keydifferences.com/difference-between-binomial-and-poisson-distribution.html)
 - [Difference between Poisson and Binomial distributions](http://math.stackexchange.com/questions/1050184/difference-between-poisson-and-binomial-distributions)
 - [Statistical Distributions (e.g. Normal, Poisson, Binomial) and their uses](http://www.healthknowledge.org.uk/public-health-textbook/research-methods/1b-statistical-methods/statistical-distributions)

![*figure 4.4.4.3 : Diagram of network among probability distributions*](figure/table-of-common-distributions.jpg)
 
 From above diagram we can know the network and relationship among probability distributions. Besides, we can try to refer to below probability distribution listing attach with R codes and examples for pratice :
 
 - [Probability Distributions in R (Stat 5101, Geyer)](http://www.stat.umn.edu/geyer/old/5101/rlook.html)

![*figure 4.4.4.4 : R functions in probability distributions*](figure/r-functions-for-prob-dist.jpg)

  Here I try to bootstrap/resampling the scores of matches of the dataset to test the Kelly model and get the mean/likelihood value. Boostrapping the scores and staking model will be falling in the following sections [4.5 Staking Ⓜodel and Ⓜoney Management] and [4.6 Expectation Ⓜaximization and Staking Simulation].

## 4.5 Staking Ⓜodel and Ⓜoney Ⓜanagement

### 4.5.1 Risk Management

```{r bank-roll2, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}
#'@ suppressAll(source('./function/buildFund.R'))
suppressAll(source('./function/plotChart.R'))
#'@ SOFund <- buildFund(dat)
#'@ saveRDS(SOFund, file = './KellyApps/data1/SOFund.rds')
SOFund <- read_rds(path = './KellyApps/data1/SOFund.rds')

## Select 1st main-fund and also 1st sub-fund.
plotFund <- plotChart(SOFund, type = 'single')
plotFund
```

*graph 4.5.1.1 : Sample data of candle stick chart for fund growth of firm A via agent A. (\$1 = \$10,000)*

<br>

  Above table shows the return rates of investment fund from firm A via agent A. We know the initial unvestment fund from `r currency(as.numeric(Op(SOFund)[1,])) * 10000` growth to `r currency(as.numeric(tail(Cl(SOFund), 1))) * 10000` within year `r row.names(data.frame(SOFund)) %>% range` with a high return rates `r percent(as.numeric(tail(Cl(SOFund), 1) / as.numeric(Op(SOFund)[1,])))`. Well, I try to apply Kelly model for stakes management which is in previous section [4.3 Kelly Ⓜodel].

  *Galema, Plantinga and Scholtens (2008)*^[You are feel free to refer to [Reference for industry knowdelege and academic research portion for the paper.] in **7.4 References** for further details] introduce [Fama-MacBeth regressions](https://en.wikipedia.org/wiki/Fama%E2%80%93MacBeth_regression)^[FM-regression is a [Capital asset pricing model (CAPM)](https://en.wikipedia.org/wiki/Capital_asset_pricing_model) which estimate the rates of return of an asset.] for portfolio management. The method estimates the [betas](https://en.wikipedia.org/wiki/Beta_coefficient) and [risk premia](https://en.wikipedia.org/wiki/Risk_premium) for any [risk factors](https://en.wikipedia.org/wiki/Risk_factor) that are expected to determine asset prices. The method works with multiple assets across time ([panel data](https://en.wikipedia.org/wiki/Panel_data)). The parameters are estimated in two steps:

  - First regress each asset against the proposed risk factors to determine that asset's beta for that risk factor.
  - Then regress all asset returns for a fixed time period against the estimated betas to determine the risk premium for each factor.
  
**Assumptions of CAPM**

All investors:

  - Aim to maximize economic utilities (Asset quantities are given and fixed).
  - Are rational and risk-averse.
  - Are broadly diversified across a range of investments.
  - Are price takers, i.e., they cannot influence prices.
  - Can lend and borrow unlimited amounts under the risk free rate of interest.
  - Trade without transaction or taxation costs.
  - Deal with securities that are all highly divisible into small parcels (All assets are perfectly divisible and liquid).
  - Have homogeneous expectations.
  - Assume all information is available at the same time to all investors.
  
  Here I skip above section and will leave it as future study due to it will estimate below point which invilve in financial and accounting :
  
  - active investors, inactive investors and potential investors.
  - rates of return of an asset (which is my previous courses in [Coursera Improving Business Finances and Operations](https://github.com/englianhu/Coursera-Improving-Business-Finances-and-Operations) but not yet complete the specialization)
  - governmental taxes.

  *Martin Spann and Bernd Skiera (2009)*^[Kindly refer to 19th paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**] applied a basic probability sets on the draw games and also the portion of win and loss. The author simply measured the portion of the draw result with win/loss to get the edge to place a bet. However it made a loss on Italian operator Oddset due to the 25% high vigorish but profitable in 12%. Secondly, the bets placed on fixed odds but not Asian Handicap and also a fixed amount $100.

<iframe width="560" height="315" src="https://www.youtube.com/embed/IH7On9hJsAk" frameborder="0" allowfullscreen></iframe>

*video 4.5.1.1 : Calculated Bets - computers, gambling, mathematical modeling to Win (part 4 of 4)*

<br>

  *table 4.3.2.1* shows a risk portfolio for every single league which is roughly similar with [**Parimutuel Betting**](https://en.wikipedia.org/wiki/Parimutuel_betting) but a portion among the initial fund required. In order to equalise the intial fund size. next section I will united it and also application of resampling method to get the optimal league risk profile.

<iframe src="https://raw.githubusercontent.com/scibrokes/kelly-criterion/d12b9b565b49f4b987470ca2cd853619f050594d/references/Kelly%20vs%20Markowitz%20Portfolio%20Optimization.pdf" style="width:560px; height:500px;" frameborder="0"></iframe>

*paper 4.5.1.1 : Magnus Erik Hvass Pedersen (2014)*

<br>

  From *table 4.5.1.1* we can know the risk of the all investmental funds. You are feel free to browse over [KellyApps](https://beta.rstudioconnect.com/content/2311/)^[*shinyapp 4.3.5.1*] for more details.
  
  In order to equalise the initial fund size, here I united it as `r currency(iniVal)` which is get from max value among the 110 funds.

```{r}
## need to embed a shinyapps for comparison of the effeciency of follow bets.
## ![Kelly fund](figure/.gif)
```

  From above shinyApps, we can know the initial fund required and the risk as well as the return of investment for us to follow the firm A with application of a time series onto the Kelly staking models.

  Now we try to simulate whole 110 Kelly funds with steps in [4.3 Kelly Ⓜodel] to made a comparison based on the portion of stakes from the initial pools for every single league as we can refer to *table 4.3.2.1*.

```{r lprofile2, echo = FALSE, results = 'asis'}
## Below profHandling() will read all league risk profile but now we only need to read lRProf1A.rds.
#'@ source('./function/profHandling.R', local = TRUE)
#'@ wProf = profHandling()
#'@ lRProf = profHandling(type = 'weight.stakes')
#'@ rm(profHandling)
source('./function/leagueRiskProf.R', local = TRUE)

if(file.exists('./data/lRProf1A.rds')) {
    lRProf1A <- read_rds(path = './data/lRProf1A.rds')
} else {
    lRProf1A <- leagueRiskProf(dat, type = 'weight.stakes', weight.type = 'all')
    saveRDS(lRProf1A, file = './data/lRProf1A.rds')
}

lRiskSettingProf <- lRProf1A %>% 
  mutate(portion = percent(all.lg/sum(all.lg)), baseline = currency(iniVal * portion))

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "League Stakes Profiling"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Staking allocation and portfolio ($0,000)")), 
  as.htmlwidget(lRiskSettingProf %>% .[sample(1:nrow(.), 6), ] %>% formattable(list(
    all.lg = color_tile('white', 'darkgoldenrod'), 
    portion = color_tile('white', 'darkgoldenrod'), 
    baseline = color_tile('white', 'darkgoldenrod')))))
```

*table 4.5.1.2* : `r paste0(dim(lRiskSettingProf), collapse = ' x ')` : *A simple new league stakes profile.*

  *table 4.5.1.2* is just a simple baseline profile for league risk management to test the Kelly models. Here I try to adjust few points to do comparison : 
  
  - united the initial fund size
  - set a baseline staking portfolio acrosss the leagues
  - application of various Kelly but not reversed models

```{r kelly-staking-models, echo = FALSE, results = 'asis'}
## After we set a united initial fund size and set a baseline for staking, here we need to test the Kelly.


```

  *张丹 (2016)*^[Kindly refer to 7th paper inside [Reference for technical research on programming and coding portion for the paper.] in **7.4 References** for further details] provides couple of r package for investment and analysis in financial market. You can also refer to [Introduction of R Packages](http://rpubs.com/englianhu/introduction-of-r-packages).

### 4.5.2 Investors' Fund Refill

  Kelly model will be a good risk averse investment portfolio. As we know normally a mutual fund or any investment fund will advise investors credit a certain money into the pool regularly. For this section I keep it as the next study which is in [Application of Kelly Criterion model in Sportsbook Investment - Part II](https://github.com/scibrokes/kelly-criterion) as an dynamic staking baseline model upon injection of new fund into the pool. It will includes the :
  
  - bonus issue or dividends
  - refill or pump-in money into the pool
  - fund management and admin fees

![*equation 4.5.2.1 : Economic Order Quantity (EOQ)*](./figure/EOQ.jpg)

  Base on above euqation, there has some criteria as below :

  - $C$ is the total cycle-inventory cost per annum which is the invest or pump in figure into the investment pool.
  - $Q$ is the fund size which pump into the investment pool. (For example: normally the investment fund or insurance company will advise the investors regularly credit a certain money into whose investment account.) 
  - $H = 1$ due to there has no holding cost per annum unless there is inactive account which will be charges a certain amount of the administration fee where it is not apply to active players.
  - $D$ is the betting stakes per annum.
  - $S = 1$ due to there has no setup costs per lot. (unless we count in the bank charges, for example : western union, Entropay, bank transfer fee, etc)

  You are feel free to know about inventory management and Kelly fund portfolio management via [Module 3: Inventory and Supply Chain Management](http://rpubs.com/englianhu/188394).

## 4.6 Expectation Ⓜaximization and Staking Simulation

### 4.6.1 Truncated Bivariate Normal Distribution

  Before I simulate all 110 Kelly weighted models 100 times by resampling the scores. Here I using few different *trancated bivariate normal distribution*^[You can refer to few r packages which are [MASS](https://cran.r-project.org/web/packages/MASS/index.html), [mvtnorm](https://cran.r-project.org/web/packages/mvtnorm/index.html) and [tmvtnorm](https://cran.r-project.org/web/packages/tmvtnorm/index.html).] and the weight parameters to get the optimal randomise scores.

  Due to there has negative lambda values which unable proceed a scores based on basic bivariate normal distribution. Here I try to build few models in order to get the best fit model for soccer scores resampling.

  - 1st bivariate normal distribution
    
    + adjust all negative values to be 0
    + measure the mean of negative values and add to all positive values as average.
    
  - 2nd bivariate normal distribution
    
    + adjust all negative values to be 0
    + measure the mean of negative values and substrated by all positive values as average.
    + multiply all values by the portion of baseline which is the raw dataset.
    
  - 3rd bivariate normal distribution
    
    + adjust all negative values to be 0
    + apply nested normal distribution on the mean and sd of all negative values and distribute to all positive values as average in `rnorm`.
    + apply looping to get the likehood values.
    
  - 4th bivariate normal distribution
    
    + apply truncated adjustment with set a set of lower and upper values.
    + set min as lower interval and max as upper interval.
    + multiply all values by the portion of baseline which is the raw dataset.
    
  - 5th bivariate normal distribution
    
    + apply truncated adjustment with set a set of lower and upper values.
    + manually set min as lower interval and `3, 2.3`^[3 goals for home team and 2.3 goals for away team]  as upper interval.
    
  - 6th bivariate normal distribution
    
    + apply truncated adjustment with set a set of lower and upper values.
    + manually set `1, 0`^[1 goal for home team and 0 goal for away team] as lower interval and `2, 2.3`^[3 goals for home team and 2.3 goals for away team]  as upper interval.
    
  - 7th bivariate normal distribution
    
    + apply truncated adjustment with set a set of lower and upper values.
    + set min values as lower interval and max as upper interval.
    + multiply all values by the portion of baseline which is the raw dataset.
    
  - 8th bivariate normal distribution
    
    + apply truncated adjustment with set a set of lower and upper values.
    + set min values as lower interval and mean values as upper interval.
    + apply looping to get the likehood values by stepwise adding 0.0001 to upper interval.
    
  - 9th bivariate normal distribution
    
    + apply truncated adjustment with set a set of lower and upper values.
    + set 1st quantile values as lower interval and 3rd quantile values as upper interval.
    + apply looping to get the likehood values by stepwise exoanding 0.0001 to both lower and upper interval.
    
  - 10th bivariate normal distribution
    
    + apply truncated adjustment with set a set of lower and upper values.
    + set mean values as lower interval and 3rd quantile values as upper interval.
    + apply looping to get the likehood values by stepwise exoanding 0.0001 to both lower and upper interval.
    
```{r test-scores, echo = FALSE, results = 'asis'}
## Test the efficiency of truncated normal distribution.
suppressMessages(library('MASS'))
#'@ source('./function/rScores.R', local = TRUE)
#'@ scores <- dat[c('FTHG', 'FTAG')]
#'@ scores1 <- rScores(dat, type = 'option1')
#'@ scores2 <- rScores(dat, type = 'option2')
#'@ scores3 <- rScores(dat, type = 'option3')
#'@ scores4 <- rScores(dat, type = 'option4')
#'@ scores5 <- rScores(dat, type = 'option5')
#'@ scores6 <- rScores(dat, type = 'option6')
#'@ scores7 <- rScores(dat, type = 'option7')
#'@ scores8 <- rScores(dat, type = 'option8')
#'@ scores9 <- rScores(dat, type = 'option9')
#'@ scores10 <- rScores(dat, type = 'option10')

## save random scoring models.
#'@ saveRDS(scores, file = './data/scores.rds')
#'@ saveRDS(scores1, file = './data/scores1.rds')
#'@ saveRDS(scores2, file = './data/scores2.rds')
#'@ saveRDS(scores3, file = './data/scores3.rds')
#'@ saveRDS(scores4, file = './data/scores4.rds')
#'@ saveRDS(scores5, file = './data/scores5.rds')
#'@ saveRDS(scores6, file = './data/scores6.rds')
#'@ saveRDS(scores7, file = './data/scores7.rds')
#'@ saveRDS(scores8, file = './data/scores8.rds')
#'@ saveRDS(scores9, file = './data/scores9.rds')
#'@ saveRDS(scores10, file = './data/scores10.rds')

## read random scoring models.
#'@ scores <- read_rds(path = './data/scores.rds')
#'@ scores1 <- read_rds(path = './data/scores1.rds')
#'@ scores2 <- read_rds(path = './data/scores2.rds')
#'@ scores3 <- read_rds(path = './data/scores3.rds')
#'@ scores4 <- read_rds(path = './data/scores4.rds')
#'@ scores5 <- read_rds(path = './data/scores5.rds')
#'@ scores6 <- read_rds(path = './data/scores6.rds')
#'@ scores7 <- read_rds(path = './data/scores7.rds')
#'@ scores8 <- read_rds(path = './data/scores8.rds')
#'@ scores9 <- read_rds(path = './data/scores9.rds')
#'@ scores10 <- read_rds(path = './data/scores10.rds')

#'@ sc0 <- lm(scores$FTHG ~ scores$FTAG)
#'@ sc1 <- lm(scores1$pred$FTHG ~ scores1$pred$FTAG)
#'@ sc2 <- lm(scores2$pred$FTHG ~ scores2$pred$FTAG)
#'@ sc3 <- lm(scores3$pred$FTHG ~ scores3$pred$FTAG)
#'@ sc4 <- lm(scores4$pred$FTHG ~ scores4$pred$FTAG)
#'@ sc5 <- lm(scores5$pred$FTHG ~ scores5$pred$FTAG)
#'@ sc6 <- lm(scores6$pred$FTHG ~ scores6$pred$FTAG)
#'@ sc7 <- lm(scores7$pred$FTHG ~ scores7$pred$FTAG)
#'@ sc8 <- lm(scores8$pred$FTHG ~ scores8$pred$FTAG)
#'@ sc9 <- lm(scores9$pred$FTHG ~ scores9$pred$FTAG)
#'@ sc10 <- lm(scores10$pred$FTHG ~ scores10$pred$FTAG)

#'@ scoresall <- rScores(dat, type = 'all')
#'@ saveRDS(scoresall, file = './data/scoresall.rds')

if(file.exists('./data/scoresall.rds')){
  scoresall <- read_rds(path = './data/scoresall.rds')
} else {
  scoresall <- rScores(dat, type = 'all')
}

lms <- llply(scoresall$opts, function(x) lm(FTHG~FTAG, x$pred))

#'@ lms$opt0 <- mvrnorm(n = nrow(dat[c('FTHG', 'FTAG')]), 
#'@                     mu = colMeans(dat[c('FTHG', 'FTAG')]), 
#'@                     Sigma = as.matrix(var(dat[c('FTHG', 'FTAG')])))

# scores <- read_rds('./data/scores.rds')
# dat[c('FTHG', 'FTAG')]
lms$opt0 <- lm(FTHG~FTAG, data = dat[c('FTHG', 'FTAG')])
```

```{r test-scores1A, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt1, main = 'option1', pch = '+')
```

*graph 4.6.1.1A : comparison of random scoring models.*

```{r test-scores1B, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt2, main = 'option2', pch = '+')
```

*graph 4.6.1.1B : comparison of random scoring models.*

```{r test-scores1C, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt3, main = 'option3', pch = '+')
```

*graph 4.6.1.1C : comparison of random scoring models.*

```{r test-scores1D, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt4, main = 'option4', pch = '+')
```

*graph 4.6.1.1D : comparison of random scoring models.*

```{r test-scores1E, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt5, main = 'option5', pch = '+')
```

*graph 4.6.1.1E : comparison of random scoring models.*

```{r test-scores1F, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt6, main = 'option6', pch = '+')
```

*graph 4.6.1.1F : comparison of random scoring models.*

```{r test-scores1G, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt7, main = 'option7', pch = '+')
```

*graph 4.6.1.1G : comparison of random scoring models.*

```{r test-scores1H, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt8, main = 'option8', pch = '+')
```

*graph 4.6.1.1H : comparison of random scoring models.*

```{r test-scores1I, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt9, main = 'option9', pch = '+')
```

*graph 4.6.1.1I : comparison of random scoring models.*

```{r test-scores1J, echo = FALSE, results = 'asis'}
par(mfrow = c(2, 4))
plot(lms$opt0, main = 'raw', pch = '+')
plot(lms$opt10, main = 'option10', pch = '+')
```

*graph 4.6.1.1J : comparison of random scoring models.*

```{r test-scores-sum, echo = FALSE, results = 'asis'}
suppressPackageStartupMessages(library('formattable'))
#'@ pstpois <- list(scores1 = scores1$pstpois, scores2 = scores2$pstpois, 
#'@                 scores3 = scores3$pstpois, scores4 = scores4$pstpois, 
#'@                 scores5 = scores5$pstpois, scores6 = scores6$pstpois, 
#'@                 scores7 = scores7$pstpois, scores8 = scores8$pstpois, 
#'@                 scores9 = scores9$pstpois, scores10 = scores10$pstpois)
#'@ rm(scores, scores1, scores2, scores3, scores4, scores5, scores6, scores7, scores8, scores9, scores10)

#'@ scs.m <- ldply(pstpois, function(x) x$mean) %>% mutate(.id = paste(.id, c('home', 'away'), sep = '.'))

scs.m <- ldply(scoresall$opts, function(x) x$pstpois$mean) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "Truncated Bivariate Normal Distribution"), 
           tags$h5(align = "center", class = "text-muted", 
                   "Mean values and comparison among randomize bivariate scoring models")), 
  as.htmlwidget(scs.m %>% formattable))
```

*table 4.5.2.1A* : `r paste0(dim(scs.m), collapse = ' x ')` : *Comparison of truncated bivariate normal distributions.*

```{r test-scores-var, echo = FALSE, results = 'asis'}
#'@ scs.v <- ldply(pstpois, function(x) x$var) %>% mutate(.id = paste(.id, c('FTHG', 'FTAG'), sep = '.'))

scs.v <- ldply(scoresall$opts, function(x) x$pstpois$var) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "Truncated Bivariate Normal Distribution"), 
           tags$h5(align = "center", class = "text-muted", 
                   "variance among randomize bivariate scoring models")), 
  as.htmlwidget(scs.v %>% formattable))
```

*table 4.5.2.1B* : `r paste0(dim(scs.v), collapse = ' x ')` : *Comparison of truncated bivariate normal distributions.*

```{r test-scores-lm, echo = FALSE, results = 'asis'}
suppressPackageStartupMessages(library('broom'))
#'@ llply(list(option0 = sc0, option1 = sc1, option2 = sc2, option3 = sc3, option4 = sc4, option5 = sc5, option6 = sc6, option7 = sc7, option8 = sc8, option9 = sc9, option10 = sc10), summary)

#'@ scs.b <- ldply(list(option0 = sc0, option1 = sc1, option2 = sc2, option3 = sc3, option4 = sc4, option5 = sc5, option6 = sc6, option7 = sc7, option8 = sc8, option9 = sc9, option10 = sc10), tidy)

scs.b <- lms %>% ldply(tidy) %>% tbl_df

tagList(
  tags$div(align = "center", 
           class = "bg-info", 
           tags$h3(class = "bg-primary", "Truncated Bivariate Normal Distribution"), 
           tags$h5(align = "center", class = "text-muted", 
                   "summary among randomize bivariate scoring models")), 
  as.htmlwidget(scs.b %>% formattable))
```

*table 4.5.2.1C* : `r paste0(dim(scs.v), collapse = ' x ')` : *Comparison of truncated bivariate normal distributions.*

```{r compare-tmv1, echo = FALSE, results = 'asis'}
suppressMessages(library('stringr'))

compare <- ldply(lms, function(x) {
    y <- summary(x)$fstatistic
    df <- data.frame(AIC = AIC(x), BIC = BIC(x), t(summary(x)$df), 'p.value' = pf(y[1], y[2], y[3], lower.tail = FALSE))
    names(df) <- c('AIC', 'BIC', 'df', 'residuals', 'df', 'p.value'); df}) %>% tbl_df

compare %<>% mutate(delta.AIC = AIC - min(AIC), Loglik.AIC = exp(-0.5 * delta.AIC), weight.AIC = Loglik.AIC/sum(Loglik.AIC), delta.BIC = BIC - min(BIC), Loglik.BIC = exp(-0.5 * delta.BIC), weight.BIC = Loglik.BIC/sum(Loglik.BIC))

compare %<>% mutate(AIC = accounting(AIC), BIC = accounting(BIC), df = accounting(df, format = 'd'), residuals = accounting(residuals, format = 'd'), p.value = percent(p.value), delta.AIC = accounting(delta.AIC), delta.BIC = accounting(delta.BIC))
```

```{r compare-tmv2, echo = FALSE, results = 'asis'}
compare %>% formattable(list(
  AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),

  BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %.0f)', x, rank(x))),
  
  df = formatter('span', style = x ~ formattable::style(color = ifelse(rank(-x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %02d)', x, rank(-x))),
  
  residuals = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.0f (rank: %02d)', x, rank(x))),
  
  p.value = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%1.2f%% (rank: %02d)', 100 * x, rank(x))),
  
  delta.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.AIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  delta.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  Loglik.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x))),
  
  weight.BIC = formatter('span', style = x ~ formattable::style(color = ifelse(rank(x) <= 3, 'blue', 'grey')), x ~ sprintf('%.4f (rank: %.0f)', x, rank(x)))

  )) %>% as.htmlwidget
```

*table 4.5.2.1D* : `r paste0(dim(compare), collapse = ' x ')` : *Comparison of truncated bivariate normal distributions.*

  From above models, we know that `r compare %>% filter(AIC == min(AIC), BIC == min(BIC)) %>% .['.id'] %>% as.character` is the best fit model and I'll apply it in further section [4.6.2 Resampling Scores and Stakes]. You are feel free to refer to below articles for further understanding:
  
  - [Introduction to Bivariate Analysis](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Introduction%20to%20Bivariate%20Analysis.pdf)
  - [How can I tell if a model fits my data?](http://www.itl.nist.gov/div898/handbook/pmd/section4/pmd44.htm)
  - [A residuals vs. fits plot](https://onlinecourses.science.psu.edu/stat501/node/36)

### 4.6.2 Resampling Scores and Stakes

  From the article [凯利模式资金管理](http://ch-hsieh.blogspot.com/2015/01/kelly-criterion-0.html)^[You might refer to [Application of Kelly Criterion model in Sportsbook Investment](https://github.com/scibrokes/kelly-criterion) as well.] we know the application of generalization of Kelly criterion for uneven payoff games.

$$G: = \mathop {\lim }\limits_{N \to \infty } \frac{1}{N}\log\left( {\frac{{{S_N}}}{{{S_0}}}} \right) \cdots equation\ 4.3.2$$

  In order to get the optimal value, I apply the bootrapping and resampling method.
  
$$L(\rho) = \prod_{i=1}^{n} (x_{i}|\rho) \cdots equation\ 4.3.4$$
  
  Now we look at abpve function from a different perspective by considering the observed values $x_{1}, x_{2}, x_{3}… x_{n}$ to be fixed *parameters* of this function, whereas $\rho$ will be the function's variable and allowed to vary freely; this function will be called the [**likelihood**](https://onlinecourses.science.psu.edu/stat414/node/191).

# 5. ®esult

  - Section [5.1 Comparison of the ®esults] - Comparison of the Returns of Staking Models.
  - Section [5.2 Ⓜarket Basket] - Analyse the Hedging or Double up Invest by Firm A.

## 5.1 Comparison of the ®esults

### 5.1.1 Comparison of Fully Followed Bets

Chapter 4.2 Comparison of Different Feature Sets and Betting Strategies in []()

```{r, echo = FALSE, results = 'asis'}
## Bootstrap and apply maximum likelihood model 
## http://www.r-bloggers.com/apple-compared-to-others-with-ggthemes/
#'@ 
#'@ kable(dat) ##knit table
```

  *Dixon and Pope (2003)* apply linear model to compare the efficiency of the odds prices offer by first three largest Firm A, B and C in UK.

### 5.1.2 Comparison of Missing Bets

  <s>Due to I have no followed bets data, therefore there has no any clue but based on my previous experience in Telebiz, Caspo and Global Solution Sdn Bhd as list in my [CV](https://beta.rstudioconnect.com/englianhu/ryo-eng/ryo-eng.html) I try to adjust setting of 90% successful following rates and 90% of average odds compare to firm A.
  
  According to above result, in order to made it workable in real life. Here I simulate whole Kelly function with monte carlo method to get the return.</s>

  Here I skip this sub-section due to no following bets dataset and somemore there is a profit sharing investment business.

## 5.2 Market Basket

  By refer to *®γσ, Eng Lian Hu (2016)*^[Kindly refer to 28th paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**], here I apply the `arules` and `arulesViz` packages to analyse the market basket of the bets.

```{r gvis-options, echo = FALSE, results = 'asis'}
## Set options back to original options
options(op)
#'@ stopImplicitCluster2()
```

# 6. Conclusion

  - Section [6.1 Conclusion] - Conclusion of this Research Paper.
  - Section [6.2 Future Works] - Future Research or Enhancement.

## 6.1 Conclusion

  Due to the data-sets I collected just one among all agents among couple sports-bookmakers [4lowin](https://www.youtube.com/watch?v=eFYS0jdSiWc). Here I cannot determine if the sample data among the population...

> JA : What skills and academic training (example: college courses) are valuable to sports statisticians?

> KW : I would say there are three sets of skills you need to be a successful sports statistician:

>  - Quantitative skills - the statistical and mathematical techniques you'll use to make sense of the data. Most kinds of coursework you'd find in an applied statistics program will be helpful. Regression methods, hypothesis testing, confidence intervals, inference, probability, ANOVA, multivariate analysis, linear and logistic models, clustering, time series, and data mining/machine learning would all be applicable. I'd include in this category designing charts, graphs, and other data visualizations to help present and communicate results.

>  - Technical skills - learning one or more statistical software systems such as R/S-PLUS, SAS, SPSS, Stata, Matlab, etc. will give you the tools to apply quantitative skills in practice. Beyond that, the more self-reliant you are at extracting and manipulating your data directly, the more quickly you can explore your data and test ideas. So being adept with the technology you're likely to encounter will help tremendously. Most of the information you'd be dealing with in sports statistics would be in a database, so learning SQL or another query language is important. In addition, mastering advanced spreadsheet skills such as pivot tables, macros, scripting, and chart customization would be useful.

>  - Domain knowledge - truly understanding the sport you want to analyze professionally is critical to being successful. Knowing the rules of the game; studying how front offices operate; finding out how players are recruited, developed, and evaluated; and even just learning the jargon used within the industry will help you integrate into the organization. You'll come to understand what problems are important to the GM and other decisionmakers, as well as what information is available, how it's collected, what it means, and what its limitations are. Also, I recommend keeping up with the discussions in your sport's analytic community so you know about the latest developments and what's considered the state of the art in the public sphere. One of the great things about being a sports statistician is getting to follow your favorite websites and blogs as a legitimate part of your job!

**source : [Preparing for a Career as a Sports Statistician: Two Interviews with People in the Field](http://stattrak.amstat.org/2012/08/01/sports-statistician/)**

  In this Part II research paper I try to add a section which is filtered out only English soccer leagues and the revenue and profit & loss all sessional based but not annum based to make it applicable to my future staking in real world. The proportional staking and also money management on the staking pools. You are feel free to browse from the content page [Betting Strategy and Model Validation](https://github.com/scibrokes/betting-strategy-and-model-validation).

**Journey to West**

  The statistical analysis on sportsbook and soccereconomics is popular in US, Europe and Ocean Pacific but not yet in Asia. Here I am learning from the western professional sportsbook consultancy firms and do shares with those who like scientific analysis on soccer sports.

  If you get interest to be a punter, you are feel free to read over below presentation paper from a British consultancy firm to know the requirement to be a professional gambler.

<iframe src="https://raw.githubusercontent.com/scibrokes/betting-strategy-and-model-validation/c2da2e5ca09aaf218616045031c9ee4ce3537b18/references/An%20Introduction%20to%20Football%20Modelling%20at%20SmartOdds%20(v1).pdf" style="width:560px; height:500px;" frameborder="0"></iframe>

<br>

![*Mark Dixon*](figure/back-to-the-future-atass-1.png)

*graph 6.1.1 : The pioneer of sportsbook statistical analytical ATASS --- founder : Mark Dixon*

<br>

  You are feel free to know further about the history of Mr Mark Dixon from [Boffins -vs- Bookies (The Man Who Broke the World Leading Bookmakers)](https://englianhu.wordpress.com/sportsbook/boffins-vs-bookies-the-man-who-broke-the-world-leading-bookmakers/).

## 6.2 Future Works

  *Niko Marttinen (2001)* has conducted a very detail and useful but also applicable betting system in real life. There has a ordered probit model which shows a high accuracy predictive model compare to his Poisson (Escore) model. Well, the *®γσ, Lian Hu ENG (2016)*^[The research modelling with testing the efficiency of odds price which had completed in year 2010. Kindly refer to 3rd paper in [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**] has build a weight inflated diagonal poisson model which is more complicated and shophitiscated and later *®γσ, Lian Hu ENG (2014)*^[Kindly refer to 4th paper inside [Reference for industry knowdelege and academic research portion for the paper.] under **7.4 References**]. However there has an automatically and systematically trading system which wrote in *VBA + S-Plus + Excel + SQL*^[the betting system has stated in his paper.] which is very useful as reference. The author use VBA to automac the algorithmic trading while there has no Asian Handicap and Goal Line odds price data to simulate compare to mine. While currently the shinyapps with RStudioConnect can also build an algorithmic trading system. However the *session timeout issue*^[The connection timeout issue might be a big issue for real time algorithmic trading] might need to consider. The [shinydashboard example](https://rstudio.github.io/shinydashboard/examples.html) from ｮStudio might probably cope with the issue.
  
  *John Fingleton & Patrick Waldron (1999)* applied Shin model to test the portion of hedge funds and smart punters. As I stated in [4.2 Linear Ⓜodel], the sparkR, RHadoop and noSQL require in order to analyse the high volume betslips dataset. Its interesting and will conduct the research if all betslips of bookmaker(s) is(are) available in the future.

  From the [4.3 Kelly Ⓜodel] we test the staking model, the *table 4.2.1* we apply the linear models and choose the best fit model based on the edge of odds price. [4.4 Poisson Ⓜodel] we try to reverse the odds price placed to get the probabilities of scoring different scores. Now we try to test the return of staking on different handicap (ex: 0, 0.25, 0.5, 0.75, 1 etc.) to know which handicap earn the most. Nowadays the hotest matches of four major leagues provides few handicaps market, there will be another case study and research to increase the profit base on same probabilities  and also edge but staking on different handicap. The dataset will be collect for research beyond the future. The effects of Win-Half and Loss-Half might probably more effective by application of Poison models since it is a descrete outcome while I take it as a known linear effects this time due to the odds price of Handicap we placed always within a range from 0.7 to 1.25.

  I will be apply Shiny to write a dynamic website to utilise the function as web based apps. I am currently conducting another research on [Analyse the Finance and Stocks Price of Bookmakers](https://github.com/scibrokes/analyse-the-finance-and-stocks-price-of-bookmakers) which is an analysis on the public listed companies and also anonymous companies revenue and profit & loss. You are welcome to refer [SHOW ME SHINY](http://www.showmeshiny.com/category/topics/sports/) and build your own shinyapps.
  
  I will also write as a package to easier load and log.

# 7. Appendices

  - Section [7.1 Documenting File Creation ] - Information of the Paper.
  - Section [7.2 Versions' Log] - Version Log of the Paper.
  - Section [7.3 Speech and Blooper] - Speech and Blooper during Conducting the Research.
  - Section [7.4 References] - Reference Papers for the Paper.

## 7.1 Documenting File Creation 

  It's useful to record some information about how your file was created.

  - File creation date: 2015-07-22
  - File latest updated date: `r Sys.Date()`
  - `r R.version.string`
  - R version (short form): `r getRversion()`
  - [**rmarkdown** package](https://github.com/rstudio/rmarkdown) version: `r packageVersion('rmarkdown')`
  - [**tufte** package](https://github.com/rstudio/tufte) version: `r packageVersion('tufte')`
  - File version: 1.0.0
  - Author Profile: [®γσ, Eng Lian Hu](https://beta.rstudioconnect.com/englianhu/ryo-eng/)
  - GitHub: [Source Code](https://github.com/Scibrokes/Betting-Strategy-and-Model-Validation)
  - Additional session information
  
```{r info, echo = FALSE, warning = FALSE, results = 'asis'}
suppressMessages(require('dplyr', quietly = TRUE))
suppressMessages(require('formattable', quietly = TRUE))

lubridate::now()
sys1 <- devtools::session_info()$platform %>% unlist %>% data.frame(Category = names(.), session_info = .)
rownames(sys1) <- NULL
sys1 %>% formattable %>% as.htmlwidget

data.frame(Sys.info = Sys.info()) %>% formattable %>% as.htmlwidget

rm(sys1)
```

<iframe width="560" height="315" src="https://www.youtube.com/embed/bFDTd4eYMCY" frameborder="0" allowfullscreen></iframe>

## 7.2 Versions' Log
  
  - File pre-release version: 0.9.0
    + file created
    + Applied **ggplot2**, **ggthemes**, **directlabels** packages for ploting. For example, the graphs applied in Section [2. Data].
  - File pre-release version: 0.9.1
    + Added [Natural Language Analysis](http://rpubs.com/englianhu/natural-language-analysis) which is research for teams' name filtering purpose.
    + Changed from **knitr::kable** to use datatble from **DT::datatable** to make the tables be dynamic.
    + Changed from **ggplot2** relevant packages to **googleVis** package to make graph dynamic.
    + Completed chapter [3. Summarise the Staking Model].
  - File pre-release version: 0.9.2 - *"2016-02-20 09:41:49 JST"*
    + Added Section [7.2 Versions' Log] and Section [7.3 Speech and Blooper] since retest the coding
    + Added Section [4. Staking Model] .
  - File pre-release version: 0.9.3 - *"2016-02-05 05:24:35 EST"*
    + Modified DT::datatable to make the documents can be save as xls/csv
    + Added log file for version upgraded
  - File pre-release version: 0.9.3.1 - *2016-06-22 13:36:33 JST*
    + Reviewed previous version, DT::datatable updated new version replaced *Button* extension from *TableTools*, removed sparkline and htmlwidget
    + Applied linear regression to test the efficiency of staking model by consultancy firm A
  - File pre-release version 0.9.4 - *2016-09-28 00:15:24 JST*
    + Added linear regression and shinyApp to test the effects on staking
  - File pre-release version 0.9.5 - *2016-12-20 02:26:19 JST*
    + Added Kelly criterion for 110 main-funds with each has 19 sub-funds.

## 7.3 Speech and Blooper

  Firstly I do appreciate those who shade me a light on my research. Meanwhile I do happy and learn from the research.

  Due to the rmarkdown file has quite some sections and titles, you might expand or collapse the codes by refer to [Code Folding and Sections](https://support.rstudio.com/hc/en-us/articles/200484568) for easier reading.

  There are quite some errors when I **knit HTML**:
  
  - let say always stuck (which is not response and consider as completed) at 29%. I tried couple times while sometimes prompt me different errors (upgrade Droplet to larger RAM memory space doesn't helps) and eventually apply `rm() and gc()` to remove the object after use and also clear the memory space.

  - Need to reload the package `suppressAll(library('networkD3'))` which in chunk `decission-tree-A` prior to apply function `simpleNetwork` while I load it in chunk `libs` at the beginning of the section 1. Otherwise cannot found that particlar function.

  - The `rCharts::rPlot()` works fine if run in chunk, but error when knit the rmarkdown file. Raised an issue : [Error : rCharts::rPlot() in rmarkdown file](http://stackoverflow.com/questions/38941927/error-rchartsrplot-in-rmarkdown-file).

  - `xtable` always shows LaTeX output but not table. Raised a question in COS : [求助！knitr Rmd pdf 中文编译 *2016年8月19日 下午9:56 7 楼*](http://cos.name/cn/topic/411119/#post-417637).Here I try other packages like [`textreg`](http://www2.uaem.mx/r-mirror/web/packages/texreg/vignettes/texreg.pdf) and [`stargazer`](http://www.princeton.edu/~otorres/NiceOutputR.pdf). You can refer to [Test version](http://englianhu.github.io/2016/08/Betting%20Strategy%20and%20Model%20Validation/Betting_Strategy_and_Model_Validation_-_Part_02test.html#linear-odel) to view the output of `stargazer` function and the source codes I reserved but added `eval = FALSE` in chunks named `lm-summary` and `lm-anova` to unexecute the codes.
  
  - I refer to [R Shiny: Rendering summary.ivreg output](http://stackoverflow.com/questions/27245173/r-shiny-rendering-summary-ivreg-output) and tried to plot the output table, but there has no bottom statistical information like *Residual standard error*, *Degree of Freedom*, *R-Squared*, *F-statistical value* and also *p-value*, therefore I use [R Shiny App for Linear Regression, Issue with Render Functions](http://stackoverflow.com/questions/33874298/r-shiny-app-for-linear-regression-issue-with-render-functions) which simply `renderPrint()` the `verbatimTextOutput()` in *shinyapp 4.2.1*.
  
  - I tried to raise an issue about post the shinyapps to RStudioConnect at [Unable publish to RStudio Connect : Error in yaml::yaml.load(enc2utf8(string), ...) : Reader error: control characters are not allowed: #81 at 276 #115](https://github.com/rstudio/rsconnect/issues/115). You might try to refer to the gif files in [#issue 115](https://github.com/rstudio/rsconnect/issues/115#issuecomment-244084859) for further information. I tried couple times and find the solution but there has no an effective solution and only allowed post to ｮPubs.com where I finally decide to seperate the dynamic shinyApp into another web url.
  
  - **Remark** : When I rewrite *Report with ShinyApps : Linear Regression Analysis on Odds Price of Stakes* and would like to post to ®StudioConnect, the wizard only allowed me post to rPubs.com (but everyone know rPubs only allow static document which is not effort to support Shinyapp). Therefore kindly refer to <https://beta.rstudioconnect.com/content/1766/>. You might download and run locally due to web base version always affected by wizards and sometimes only view datatable but sometimes only can view googleVis while sometimes unable access.
  
  - [Using formattable and plotly simultaneously](http://stackoverflow.com/questions/39319427/using-formattable-and-plotly-simultaneously) and [Possible namespace issue with plotly::last_plot() #41](https://github.com/renkun-ken/formattable/issues/41#issuecomment-171590677) solved the formattable issue.

  - The analysis in Part I might slightly different with Part II due to the timezone issue.
  
    + The daily financial settlement time is EST 0000 or HKT 1200.
    + The filtered data for observation in Part II for statistical analysis purpose while Part I is just summarise and breakdown the bets which includes all bets.

  - I am currently work as a customer service operator and self research as a smart punter. Hope my sportsbook hedge fund company website **Scibrokes®** running business soon...

![**Terminator II**](figure/Ill-Be-Back-Terminator.gif)

## 7.4 References

####  Reference for industry knowdelege and academic research portion for the paper.

  1. [**Creating a Profitable Betting Strategy for Football by Using Statistical Modelling** *by Niko Marttinen (2006)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Creating%20a%20Profitable%20Betting%20Strategy%20for%20Football%20by%20Using%20Statistical%20Modelling.pdf)
  2. [**What Actually Wins Soccer Matches: Prediction of the 2011-2012 Premier League for Fun and Profit** *by Jeffrey Alan Logan Snyder (2013)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/What%20Actually%20Wins%20Soccer%20Matches%20-%20Prediction%20of%20the%202011-2012%20Premier%20League%20for%20Fun%20and%20Profit.pdf)
  3. [**Odds Modelling and Testing Inefficiency of Sports Bookmakers : Rmodel** by ®γσ, Eng Lian Hu (2016)](https://github.com/scibrokes/odds-modelling-and-testing-inefficiency-of-sports-bookmakers/blob/master/Odds%20Modelling%20and%20Testing%20Inefficiency%20of%20Sports-Bookmakers.pdf)
  4. [**Apply Kelly-Criterion on English Soccer 2011/12 to 2012/13** *by ®γσ, Eng Lian Hu (2014)*](https://github.com/scibrokes/kelly-criterion)
  5. [**The Betting Machine** *by Martin Belgau Ellefsrød (2013)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/The%20Betting%20Machine.pdf)
  6. [**The Kelly Criterion in Blackjack Sports Betting, and the Stock Market** *by Edward Thorp (2016)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/The%20Kelly%20Criterion%20in%20Blackjack%20Sports%20Betting%2C%20and%20the%20Stock%20Market.pdf)
  7. [**Statistical Methodology for Profitable Sports Gambling** *by Fabián Enrique Moya (2012)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Statistical%20Methodology%20for%20Profitable%20Sports%20Gambling.pdf)
  8. [**How to apply the Kelly criterion when expected return may be negative?** *by user1443 (2011)*](http://quant.stackexchange.com/questions/2500/how-to-apply-the-kelly-criterion-when-expected-return-may-be-negative)
  9. [**Money Management Using The Kelly Criterion** *by Justin Kuepper*](http://www.investopedia.com/articles/trading/04/091504.asp)
  10. [**Optimal Exchange Betting Strategy For WIN-DRAW-LOSS Markets** *by Darren O'Shaughnessy (2012)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Optimal%20Exchange%20Betting%20Strategy%20For%20WIN-DRAW-LOSS%20Markets.pdf)
  11. [**Kelly criterion with more than two outcomes** *by David Speyer (2014)*](http://math.stackexchange.com/questions/662104/kelly-criterion-with-more-than-two-outcomes)
  12. [**凯利模式资金管理** *by Chung-Han Hsieh (2015)*](http://ch-hsieh.blogspot.com/2015/01/kelly-criterion-0.html)
  13. [**Optimal Determination of Bookmakers' Betting Odds: Theory and Tests** *by John Fingleton & Patrick Waldron (1999)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Optimal%20Determination%20of%20Bookmakers'%20Betting%20Odds%20-%20Theory%20and%20Tests.pdf)
  14. [**Optimal Pricing in the Online Betting Market** *by Maurizio Montone (2015)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Optimal%20Pricing%20in%20the%20Online%20Betting%20Market.pdf)
  15. [**Why are Gambling Markets Organised so Differently from Financial Markets?** *by Steven Levitt (2004)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Why%20are%20Gambling%20Markets%20Organised%20so%20Differently%20from%20Financial%20Markets.pdf)
  16. [**Forecasting Accuracy and Line Changes in the NFL and College Football Betting Markets** *by Steven Xu (2013)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Forecasting%20Accuracy%20and%20Line%20Changes%20in%20the%20NFL%20and%20College%20Football%20Betting%20Markets.pdf)
  17. [**The Forecast Ability of the Dispersion of Bookmaker Odds** *by Kwinten Derave (2013-2014)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/The%20Forecast%20Ability%20of%20the%20Dispersion%20of%20Bookmaker%20Odds.pdf)
  18. [**The Stocks at Stake: Return and Risk in Socially Responsible Investment** *by Galema, Plantinga and Scholtens (2008)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/The%20Stocks%20at%20Stake%20-%20Return%20and%20Risk%20in%20Socially%20Responsible%20Investment.pdf)
  19. [**A Comparison of the Forecast Accuracy of Prediction Markets, Betting Odds and Tipsters** *by Martin Spann and Bernd Skiera (2009)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/A%20Comparison%20of%20the%20Forecast%20Accuracy%20of%20Prediction%20Markets%20Betting%20Odds%20and%20Tipsters.pdf)
  20. [**Efficiency of the Market for Racetrack Betting** *by Donald Hausch, William Ziemba and Mark Rubinstein (1981)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Efficiency%20of%20the%20Market%20for%20Racetrack%20Betting.pdf)
  21. [**Betting Market Efficient at Premiere Racetracks** *by Marshall Gramm (2011)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Betting%20Market%20Efficient%20at%20Premiere%20Racetracks.pdf)
  22. [**Late Money and Betting Market Efficiency: Evidence from Australia** *by Marshall Gramm, Nicholas McKinney and Randall Parker (2012)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Late%20Money%20and%20Betting%20Market%20Efficiency%20-%20Evidence%20from%20Australia.pdf)
  23. [**An introduction to football modelling at Smartodds** *by Robert Johnson (2011)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/An%20Introduction%20to%20Football%20Modelling%20at%20SmartOdds%20(v1).pdf)
  24. [**The Value of Statistical Forecasts in the UK Association Football Betting Market** *by Dixon and Pope (2003)*](https://github.com/scibrokes/odds-modelling-and-testing-inefficiency-of-sports-bookmakers/blob/master/reference/DixonPope2004.pdf)
  25. [**Modelling Association Football Scores and Inefficiencies in the Football Betting Market** *by Dixon & Coles (1996)*](https://github.com/scibrokes/odds-modelling-and-testing-inefficiency-of-sports-bookmakers/blob/master/reference/DixonColes1996.pdf)
  26. [**A New Interpretation of Information Rate** *by John Kelly (1956)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/A%20New%20Interpretation%20of%20Information%20Rate.pdf)
  27. [**Dynamic Modelling and Prediction of English Football League Matches for Betting** *by Crowder, Dixon, Ledford and Robinson (2001)*](https://github.com/scibrokes/odds-modelling-and-testing-inefficiency-of-sports-bookmakers/blob/master/reference/DixonLedfordRobinson2001.pdf)
  28. [**Pattern Discovery in Data Mining Programming Assignment: Frequent Itemset Mining Using Apriori** *by ®γσ, Eng Lian Hu (2016)*](http://rpubs.com/englianhu/pattern-discovery-in-data-mining-assignment1)
  29. [**Efficiency of the Racetrack Betting Market (2008 Preface Edition)** *by Donald Hausch, Victor Lo and William Ziemba (2008)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Efficiency%20of%20the%20Racetrack%20Betting%20Market%20(2008%20Edition).pdf)
  30. [**Racetrack Betting and Consensus of Subjective Probabilities** *by Lawrence Brown and Yi Lin (2002)*](https://github.com/scibrokes/betting-strategy-and-model-validation/blob/master/references/Racetrack%20Betting%20and%20Consensus%20of%20Subjective%20Probabilities.pdf)

####  Reference for technical research on programming and coding portion for the paper.

  1. [**Wrangling F1 Data With R** *by Tony Hirst (2014)*](http://www.r-bloggers.com/wrangling-f1-data-with-r-f1datajunkie-book/)
  2. [**Interactive visualizations with R - a minireview** *by Juuso Parkkinen (2014)*](http://ouzor.github.io/blog/2014/11/21/interactive-visualizations.html)
  3. [**R + htmlwidgets + DT + sparkline** *by Matthew Leonawicz (2015)*](https://blog.snap.uaf.edu/2015/10/01/r-htmlwidgets-dt-sparkline/)
  4. [**Programming Assignment 2 Submission : Data Visualization by University of Illinois at Urbana-Champaign** *by ®γσ, Eng Lian Hu (2016)*](https://beta.rstudioconnect.com/englianhu/Programming-Assignment-2-Submission/#read-data)
  5. [Using Roles via googleVis](https://cran.r-project.org/web/packages/googleVis/vignettes/Using_Roles_via_googleVis.html)
  6. [**Welcome to the Highcharter homepage** *by Joshua Kunst (2016)*](http://jkunst.com/highcharter/index.html)
  7. [**R语言量化投资常用包总结** *by 张丹(2016)*](http://mp.weixin.qq.com/s?__biz=MzA3MTM3NTA5Ng==&mid=2651054987&idx=1&sn=11c6bb68dbb0d77598a1d2459cff6dcf&chksm=84d9c21cb3ae4b0ade8f398760e6414be06c4e9cb69d1389df46e326fd481e3320c9ffb92319&scene=0#wechat_redirect)

**Powered by - Copyright® Intellectual Property Rights of <img src='figure/oda-army.jpg' width='24'> [Scibrokes®](http://www.scibrokes.com)個人の経営企業**
